{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce43c6ad-4b6f-4ec4-a30b-890bca928dbe",
   "metadata": {},
   "source": [
    "Vector Database FAISS/ChromaDB/PineCone?\n",
    "\n",
    "-  Step 1: Load sec1 and sec2 PDFs\n",
    "-  Step 2: Split into Chunks + Tag with Metadata ( Page Number + sec 2/sec1 label)\n",
    "-  Step 3: Combine into VectorDB\n",
    "\n",
    "Each Query Should Return:\n",
    "- Page Number + Sec 2/Sec1 source\n",
    "- UI should be able to filter between sec 1 and sec 2 content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825bc95-0ad0-474e-901b-c8a26a252a2c",
   "metadata": {},
   "source": [
    "langchain==0.3.9\n",
    "langchain-community==0.3.1\n",
    "pypdf==4.2.0\n",
    "python-dotenv\n",
    "faiss-cpu==1.7.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c03f34-2781-4d36-a2b7-c01fe0d8b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if a specific variable is loaded\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f027b-7b05-4403-a358-ebff9a846d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Save the full text from each PDF\n",
    "with open('full_text_from_pdfs.txt', 'w', encoding='utf-8') as full_text_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        full_text_file.write(f\"--- {label} ---\\n\")\n",
    "        for page in pages:\n",
    "            # Write the full text from each page into the file\n",
    "            full_text_file.write(page.page_content + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc4d13-fc53-4227-b928-c4c7b32d044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this part only to reload documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d5f11bd-6674-4b82-8536-674433abb4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Verify the PDF loading process\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            documents.append({\n",
    "                \"text\": page.page_content,\n",
    "                \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "#delete this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e703055d-da06-4128-b1e8-a901f2dee010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Ensure documents are stored as Document objects\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            # Create a Document object with the correct structure\n",
    "            doc = Document(\n",
    "                page_content=page.page_content,\n",
    "                metadata={\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785feec8-0a8b-4dde-b330-fbc1f3443058",
   "metadata": {},
   "source": [
    "## Trying different Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b824-7c59-40f7-8236-5b3f706e55cc",
   "metadata": {},
   "source": [
    "1. Recursive Character\n",
    "2. Paragraph\n",
    "3. Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9483d1-4376-47da-91da-b711d2815679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "# Save the chunks into a file for observation\n",
    "with open('text_chunks_recursive.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            splits = text_splitter.split_text(text)  # Split the text into chunks\n",
    "            for split in splits:\n",
    "                chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write the chunk text into the file\n",
    "                chunk_file.write(f\"--- Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a8fd6a-ab4f-4b47-8b77-866393ef42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Chunking text based on paragraph (assuming each paragraph is separated by a new line)\n",
    "paragraph_chunks = []\n",
    "\n",
    "with open('paragraph_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            # Split based on new lines (assumed paragraph boundary)\n",
    "            splits = re.split(r'\\n\\n+', text)  # Split by double newlines\n",
    "            for split in splits:\n",
    "                paragraph_chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write each paragrapbh into the file\n",
    "                chunk_file.write(f\"--- Paragraph Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bf4e5-f6c7-4eab-8d17-f6b111269ef2",
   "metadata": {},
   "source": [
    "document check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9820dd93-ea61-452c-8fc9-7a3360b526b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents list is empty!\n",
      "Embeddings generated correctly!\n"
     ]
    }
   ],
   "source": [
    "# Ensure documents list is populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n",
    "# Generate embeddings for the documents to check if embeddings are created correctly\n",
    "embedding_texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "# Generate embeddings using OpenAI embeddings model\n",
    "embeddings_vectors = embeddings.embed_documents(embedding_texts)\n",
    "\n",
    "# Check if embeddings were generated correctly\n",
    "if len(embeddings_vectors) == len(embedding_texts):\n",
    "    print(\"Embeddings generated correctly!\")\n",
    "else:\n",
    "    print(f\"Error: Expected {len(embedding_texts)} embeddings, but got {len(embeddings_vectors)}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696b21a-c247-442a-a15d-b0de002ecac4",
   "metadata": {},
   "source": [
    "## Vector Database Creation (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25facdf3-c905-428b-9860-d787ad3df3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append({\n",
    "            \"text\": page.page_content,\n",
    "            \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "        })\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    splits = text_splitter.split_text(doc[\"text\"])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Save VectorDB\n",
    "faiss_index.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Vector database created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a228b3ac-3c88-4462-a832-e0c77ed8f1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic_text_chunks.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m chunk_file:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m semantic_chunk \u001b[38;5;129;01min\u001b[39;00m semantic_chunks:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# Write the chunk's content and metadata to the file\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Semantic Chunk from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msemantic_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemantic_chunk\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(semantic_chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Add a blank line between chunks\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'source'"
     ]
    }
   ],
   "source": [
    "#rerun this one for results.\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Load the PDFs and create documents list\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append(Document(page_content=page.page_content, metadata={\"page\": page.metadata[\"page\"], \"source\": label}))\n",
    "\n",
    "# Initialize the OpenAI Embeddings model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for semantic_chunk in semantic_chunks:\n",
    "        # Write the chunk's content and metadata to the file\n",
    "        chunk_file.write(f\"--- Semantic Chunk from {semantic_chunk.metadata['source']} (Page {semantic_chunk.metadata['page']}) ---\\n\")\n",
    "        chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "        chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92c2146b-df36-4d7c-b952-a79c9a5dfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    chunk_index = 0  # Initialize chunk index\n",
    "    for doc in documents:\n",
    "        # Get the number of chunks for the current document\n",
    "        doc_chunks = [chunk for chunk in semantic_chunks if chunk.page_content in doc.page_content]\n",
    "        \n",
    "        # Add each chunk with the corresponding metadata\n",
    "        for semantic_chunk in doc_chunks:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'Unknown')\n",
    "            \n",
    "            # Write the chunk's content and metadata to the file\n",
    "            chunk_file.write(f\"--- Semantic Chunk from {source} (Page {page}) ---\\n\")\n",
    "            chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "            chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "            chunk_index += 1  # Increment the chunk index\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4fafb-6c81-457c-b0a5-3d9c3f576baa",
   "metadata": {},
   "source": [
    "## Testing Response from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a64a161-96e4-46ee-8f9a-c05905c57de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_vector_store(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Given a user's question and any provided filters (student age group, historical timeframe, theme), please:\n",
    "\n",
    "1. Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective.\n",
    "For each perspective, provide citations in page number\n",
    "\n",
    "2. Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "3. If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "4. After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "    \n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    \n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs} \n",
    "\n",
    "# Load FAISS index\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings,allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1b2c108-8856-4dee-a308-e9f3b826b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Sir Stamford Raffles\n",
      "Summary: Some argue that Raffles should be considered the founder of Singapore due to his role in signing the 1819 Treaty that allowed the British to establish a trading post in the region. His contributions to the early development of Singapore are significant and well-documented.\n",
      "\n",
      "Perspective 2: William Farquhar\n",
      "Summary: Others believe that Farquhar should be recognized as the founder of Singapore because of his efforts in building the settlement from scratch. Farquhar played a crucial role in the early development of Singapore alongside Raffles.\n",
      "\n",
      "Perspective 3: John Crawfurd\n",
      "Summary: Some may consider Crawfurd as the founder of Singapore because he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the entire island. His diplomatic efforts and contributions to the British colonial presence in Singapore are noteworthy.\n",
      "\n",
      "Discussion Questions:\n",
      "1. How do the different perspectives on who founded Singapore reflect the complexities of historical narratives and the importance of considering multiple viewpoints?\n",
      "2. In what ways do the contributions of Raffles, Farquhar, and Crawfurd to the founding of Singapore highlight the collaborative nature of historical events and the role of various individuals in shaping history?\n",
      "3. How might the differing perspectives on the founder of Singapore influence our understanding of the city-state's identity and historical legacy?\n",
      "\n",
      "Referenced sources: ['Sec1', 'Sec1', 'Sec1', 'Sec1', 'Sec1', 'Sec1', 'Sec1', 'Sec1', 'Sec1', 'Sec1']\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"Who is the founder of Singapore?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])\n",
    "print()\n",
    "print(f\"Referenced sources: {[doc.metadata['source'] for doc in response['context']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4016b3ea-ff99-4590-9f46-5fe10575d71c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Sir Stamford Raffles\n",
      "Summary: Some argue that Raffles should be considered the founder of Singapore due to his role in signing the 1819 Treaty that allowed the British to establish a trading post in the region. His contributions to the early development of Singapore are significant and well-documented.\n",
      "\n",
      "Perspective 2: William Farquhar\n",
      "Summary: Others believe that Farquhar should be recognized as the founder of Singapore because of his efforts in building the settlement from scratch. Farquhar played a crucial role in the early development of Singapore alongside Raffles.\n",
      "\n",
      "Perspective 3: John Crawfurd\n",
      "Summary: Some may consider Crawfurd as the founder of Singapore because he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the entire island. His diplomatic efforts and contributions to the British colonial presence in Singapore are noteworthy.\n",
      "\n",
      "Discussion Questions:\n",
      "1. How do the different perspectives on who founded Singapore reflect the complexities of historical narratives and the importance of considering multiple viewpoints?\n",
      "2. In what ways do the contributions of Raffles, Farquhar, and Crawfurd to the founding of Singapore highlight the collaborative nature of historical events and the role of various individuals in shaping history?\n",
      "3. How might the differing perspectives on the founder of Singapore influence our understanding of the city-state's identity and historical legacy?\n",
      "\n",
      "Referenced sources:\n",
      "Page 4 (Source: Sec1):\n",
      "of Singapore, who do you think founded Singapore? Before answering this question, think about what it \n",
      "means to be a founder. What does it mean to found a place?\n",
      "Some maintain that Raffles was the founder as he signed the 1819 Treaty that allowed the British to \n",
      "set up a trading post in the southern part of Singapore. Others argue that Farquhar was the founder of \n",
      "Singapore as he did the work of building Singapore from scratch. Some may even consider Crawfurd the\n",
      "\n",
      "Page 75 (Source: Sec1):\n",
      "means to be a founder. What does it mean to found a place?\n",
      "Some maintain that Raffles was the founder as he signed the 1819 Treaty that allowed the British to \n",
      "set up a trading post in the southern part of Singapore. Others argue that Farquhar was the founder of \n",
      "Singapore as he did the work of building Singapore from scratch. Some may even consider Crawfurd the \n",
      "founder as he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the  \n",
      "whole island.\n",
      "\n",
      "Page 75 (Source: Sec1):\n",
      "whole island.\n",
      "Read the sources that follow and conduct your own investigation into who founded Singapore.Who founded Singapore?CASE INVESTIGATION\n",
      "\n",
      "Page 4 (Source: Sec1):\n",
      "founder as he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the  \n",
      "whole island.\n",
      "Read the sources that follow and conduct your own investigation into who founded Singapore.Who founded Singapore?CASE INVESTIGATION\n",
      "SINGAPORE’S DEVELOPMENT AS A PORT CITY UNDER THE BRITISH (1819–1942)In Chapter 2, you learnt that historians often examine the causes of key historical events. This is not always\n",
      "\n",
      "Page 75 (Source: Sec1):\n",
      "66\n",
      "FROM TEMASEK TO SINGAPORE (1299–EARLY 1800 S)Today, Raffles’ name can be seen all over Singapore: Raffles Place MRT station and Raffles Boulevard, just \n",
      "to give a few examples. This is because Raffles is often recognised as the founder of Singapore. However, \n",
      "not everyone takes this view.\n",
      "Given what you have learnt about the contributions of Raffles, Farquhar and Crawfurd to the development \n",
      "of Singapore, who do you think founded Singapore? Before answering this question, think about what it\n",
      "\n",
      "Page 80 (Source: Sec1):\n",
      "71\n",
      "HOW DID SINGAPORE BECOME A BRITISH TRADING POST?Given your findings, who do you think founded Singapore? Create an inscription for a plinth, which is a \n",
      "base supporting a statue, for that person. A sentence starter has been provided below to help you in creating \n",
      "the inscription for the person who you think founded Singapore.\n",
      "Who founded Singapore?\n",
      " Statue\n",
      " Plinth\n",
      "\n",
      "Page 166 (Source: Sec1):\n",
      "Founded \n",
      "1823\n",
      "Founder  \n",
      "Sir Stamford Raffles Raffles Institution on Bras Basah Road, c. 1920\n",
      "Catholic High School  \n",
      "Founded \n",
      "1935\n",
      "Founder  \n",
      "Reverend Father Edward Becheras Catholic High School on Queen Street, c. 1936\n",
      "157\n",
      "WHAT ROLE DID THE PEOPLE IN SINGAPORE PLAY IN ITS DEVELOPMENT AS A PORT CITY FROM 1819 TO 1942?\n",
      "\n",
      "Page 78 (Source: Sec1):\n",
      "69\n",
      "HOW DID SINGAPORE BECOME A BRITISH TRADING POST?\n",
      "Crawfurd cannot be considered a founder or co-founder of the colonial settlement. It was Raffles and Farquhar who laid \n",
      "the foundations in January–February 1819, and could at least lay claim to this distinction, as reflected in the inscription \n",
      "on the base of Raffles’ statue in Westminster Abbey, London, and that on Farquhar’s grave in Perth, Scotland.\n",
      "\n",
      "Page 63 (Source: Sec1):\n",
      "– Adapted from A History of Modern Singapore, 1819–2005 by British historian Constance Mary Turnbull\n",
      "Singapore seemed ideal. While the southwestern bank of the river was swampy, the ground on the northeastern side \n",
      "was level and firm. There was an abundance of drinking water , and the river mouth formed a natural sheltered harbour .  \n",
      "Singapore was conveniently placed as a centre for trade with the eastern archipelago and only a few miles from the main\n",
      "\n",
      "Page 165 (Source: Sec1):\n",
      "Founded \n",
      "1906\n",
      "Founder  \n",
      "Singapore Hokkien Huay Kuan Tao Nan School on Armenian Street, 1910\n",
      "156\n",
      "SINGAPORE’S DEVELOPMENT AS A PORT CITY UNDER THE BRITISH (1819–1942) Map 4.3: Present-day map showing the locations of some of the early schools in Singapore\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])\n",
    "print()\n",
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    print(f\"Page {doc.metadata['page']} (Source: {doc.metadata['source']}):\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00612d7-2094-4f7a-84ba-f2477c97ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this later at home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec5fb8-32c9-494f-b8a5-17151cc7bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "csv_file = \"roots_sg_articles_cleaned.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create documents from CSV data\n",
    "articles = []\n",
    "for index, row in df.iterrows():\n",
    "    articles.append(Document(\n",
    "        page_content=row['text'],\n",
    "        metadata={\n",
    "            'title': row['title'],\n",
    "            'source': row['source'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Step 2: Load and Process PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "pdf_documents = []\n",
    "\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        pdf_documents.append({\n",
    "            \"text\": page.page_content,\n",
    "            \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "        })\n",
    "\n",
    "# Combine both article and PDF documents into one list\n",
    "all_documents = articles + pdf_documents\n",
    "\n",
    "# Step 3: Chunk the Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_documents:\n",
    "    splits = text_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        all_chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "\n",
    "# Step 4: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Step 5: Create the Combined FAISS Vector Store\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in all_chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in all_chunks]\n",
    ")\n",
    "\n",
    "# Step 6: Save the Combined Vector Store\n",
    "faiss_index.save_local(\"combined_faiss_index\")\n",
    "\n",
    "print(\"Combined vector store created and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d56d22-78a4-4233-8af3-ebc1b8b863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to query from the vector store\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Given a user's question and any provided filters (student age group, historical timeframe, theme), please:\n",
    "\n",
    "1. Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective.\n",
    "For each perspective, provide citations in page number\n",
    "\n",
    "2. Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "3. If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "4. After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

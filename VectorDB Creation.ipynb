{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce43c6ad-4b6f-4ec4-a30b-890bca928dbe",
   "metadata": {},
   "source": [
    "Vector Database FAISS/ChromaDB/PineCone?\n",
    "\n",
    "-  Step 1: Load sec1 and sec2 PDFs\n",
    "-  Step 2: Split into Chunks + Tag with Metadata ( Page Number + sec 2/sec1 label)\n",
    "-  Step 3: Combine into VectorDB\n",
    "\n",
    "Each Query Should Return:\n",
    "- Page Number + Sec 2/Sec1 source\n",
    "- UI should be able to filter between sec 1 and sec 2 content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825bc95-0ad0-474e-901b-c8a26a252a2c",
   "metadata": {},
   "source": [
    "langchain==0.3.9\n",
    "langchain-community==0.3.1\n",
    "pypdf==4.2.0\n",
    "python-dotenv\n",
    "faiss-cpu==1.7.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c03f34-2781-4d36-a2b7-c01fe0d8b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if a specific variable is loaded\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f027b-7b05-4403-a358-ebff9a846d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Save the full text from each PDF\n",
    "with open('full_text_from_pdfs.txt', 'w', encoding='utf-8') as full_text_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        full_text_file.write(f\"--- {label} ---\\n\")\n",
    "        for page in pages:\n",
    "            # Write the full text from each page into the file\n",
    "            full_text_file.write(page.page_content + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc4d13-fc53-4227-b928-c4c7b32d044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this part only to reload documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d5f11bd-6674-4b82-8536-674433abb4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Verify the PDF loading process\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            documents.append({\n",
    "                \"text\": page.page_content,\n",
    "                \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "#delete this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e703055d-da06-4128-b1e8-a901f2dee010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Ensure documents are stored as Document objects\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            # Create a Document object with the correct structure\n",
    "            doc = Document(\n",
    "                page_content=page.page_content,\n",
    "                metadata={\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785feec8-0a8b-4dde-b330-fbc1f3443058",
   "metadata": {},
   "source": [
    "## Trying different Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b824-7c59-40f7-8236-5b3f706e55cc",
   "metadata": {},
   "source": [
    "1. Recursive Character\n",
    "2. Paragraph\n",
    "3. Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9483d1-4376-47da-91da-b711d2815679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "# Save the chunks into a file for observation\n",
    "with open('text_chunks_recursive.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            splits = text_splitter.split_text(text)  # Split the text into chunks\n",
    "            for split in splits:\n",
    "                chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write the chunk text into the file\n",
    "                chunk_file.write(f\"--- Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a8fd6a-ab4f-4b47-8b77-866393ef42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Chunking text based on paragraph (assuming each paragraph is separated by a new line)\n",
    "paragraph_chunks = []\n",
    "\n",
    "with open('paragraph_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            # Split based on new lines (assumed paragraph boundary)\n",
    "            splits = re.split(r'\\n\\n+', text)  # Split by double newlines\n",
    "            for split in splits:\n",
    "                paragraph_chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write each paragrapbh into the file\n",
    "                chunk_file.write(f\"--- Paragraph Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bf4e5-f6c7-4eab-8d17-f6b111269ef2",
   "metadata": {},
   "source": [
    "document check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9820dd93-ea61-452c-8fc9-7a3360b526b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 436\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate embeddings for the documents to check if embeddings are created correctly\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m embedding_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Generate embeddings using OpenAI embeddings model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m embeddings_vectors \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39membed_documents(embedding_texts)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Ensure documents list is populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n",
    "# Generate embeddings for the documents to check if embeddings are created correctly\n",
    "embedding_texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "# Generate embeddings using OpenAI embeddings model\n",
    "embeddings_vectors = embeddings.embed_documents(embedding_texts)\n",
    "\n",
    "# Check if embeddings were generated correctly\n",
    "if len(embeddings_vectors) == len(embedding_texts):\n",
    "    print(\"Embeddings generated correctly!\")\n",
    "else:\n",
    "    print(f\"Error: Expected {len(embedding_texts)} embeddings, but got {len(embeddings_vectors)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0978f7ea-ddc4-4144-a106-e5e8f0e6cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the combined documents (articles + PDF pages) to a file\n",
    "with open('textbooks.pkl', 'wb') as f:\n",
    "    pickle.dump(documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486500d2-d435-4048-870d-499315c415c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a228b3ac-3c88-4462-a832-e0c77ed8f1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic_text_chunks.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m chunk_file:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m semantic_chunk \u001b[38;5;129;01min\u001b[39;00m semantic_chunks:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# Write the chunk's content and metadata to the file\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Semantic Chunk from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msemantic_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemantic_chunk\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(semantic_chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Add a blank line between chunks\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'source'"
     ]
    }
   ],
   "source": [
    "#rerun this one for results.\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Load the PDFs and create documents list\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append(Document(page_content=page.page_content, metadata={\"page\": page.metadata[\"page\"], \"source\": label}))\n",
    "\n",
    "# Initialize the OpenAI Embeddings model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for semantic_chunk in semantic_chunks:\n",
    "        # Write the chunk's content and metadata to the file\n",
    "        chunk_file.write(f\"--- Semantic Chunk from {semantic_chunk.metadata['source']} (Page {semantic_chunk.metadata['page']}) ---\\n\")\n",
    "        chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "        chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92c2146b-df36-4d7c-b952-a79c9a5dfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    chunk_index = 0  # Initialize chunk index\n",
    "    for doc in documents:\n",
    "        # Get the number of chunks for the current document\n",
    "        doc_chunks = [chunk for chunk in semantic_chunks if chunk.page_content in doc.page_content]\n",
    "        \n",
    "        # Add each chunk with the corresponding metadata\n",
    "        for semantic_chunk in doc_chunks:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'Unknown')\n",
    "            \n",
    "            # Write the chunk's content and metadata to the file\n",
    "            chunk_file.write(f\"--- Semantic Chunk from {source} (Page {page}) ---\\n\")\n",
    "            chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "            chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "            chunk_index += 1  # Increment the chunk index\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696b21a-c247-442a-a15d-b0de002ecac4",
   "metadata": {},
   "source": [
    "## Vector Database Creation Attempts (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25facdf3-c905-428b-9860-d787ad3df3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully!\n"
     ]
    }
   ],
   "source": [
    "# this code is for only creation from the PDF textbooks\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append({\n",
    "            \"text\": page.page_content,\n",
    "            \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "        })\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    splits = text_splitter.split_text(doc[\"text\"])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Save VectorDB\n",
    "faiss_index.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Vector database created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0a6cd32-5856-4b82-9430-2e2aa58fcca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully!\n"
     ]
    }
   ],
   "source": [
    "# second attempt for combined vectorDB creation\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "csv_file = \"roots_sg_articles_cleaned.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Fill NaN values in 'text' column with an empty string or a default string\n",
    "df['text'] = df['text'].fillna('missing content')\n",
    "\n",
    "# Ensure 'text' column is a string\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Create documents from CSV data\n",
    "articles = []\n",
    "for index, row in df.iterrows():\n",
    "    articles.append(Document(\n",
    "        page_content=row['text'],\n",
    "        metadata={\n",
    "            'title': row['title'],\n",
    "            'source': row['source'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Load the processed documents from the pickle file\n",
    "with open('textbooks.pkl', 'rb') as f:\n",
    "    pdf_documents = pickle.load(f)\n",
    "\n",
    "# # Load PDFs\n",
    "# pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "# documents = []\n",
    "\n",
    "# for label, path in pdf_paths.items():\n",
    "#     loader = PyPDFLoader(path)\n",
    "#     pages = loader.load()\n",
    "#     for page in pages:\n",
    "#         documents.append({\n",
    "#             \"text\": page.page_content,\n",
    "#             \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "#         })\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=100\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "# Combine both article and PDF documents into one list\n",
    "all_documents = articles + pdf_documents\n",
    "\n",
    "# Process the documents (both articles and PDFs)\n",
    "for doc in all_documents:\n",
    "    # Access the content using dot notation, not dict-style indexing\n",
    "    splits = text_splitter.split_text(doc.page_content)  # Access the content using .page_content\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata  # Access the metadata using .metadata\n",
    "        })\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Save VectorDB\n",
    "faiss_index.save_local(\"faiss_index_full\")\n",
    "\n",
    "print(\"Vector database created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fadf7d6-b0aa-4a33-b4af-925f5aa942e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4fafb-6c81-457c-b0a5-3d9c3f576baa",
   "metadata": {},
   "source": [
    "## Testing Response from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a64a161-96e4-46ee-8f9a-c05905c57de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_vector_store(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective. For each perspective, include a source citation in one of the following formats:\n",
    "\n",
    "Page Number (if the source is a book or document with specific page references),\n",
    "Website Link (if the source is a digital resource or website),\n",
    "Or both if applicable (e.g., a book citation with a page number and a link to the digital source).\n",
    "Format the answer as follows:\n",
    "\n",
    "Perspective #: [Answer summary]\n",
    "Page: [Page Number], Book Title: Sec1 or Sec2\n",
    "OR\n",
    "Website Link: [Link to the source]\n",
    "OR\n",
    "Page: [Page Number] | Website Link: [Link to the source]\n",
    "Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "\n",
    "If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "\n",
    "After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture. Each citation should be appropriately linked to the perspective it corresponds to, whether it is a page number, website link, or both.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "    \n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    \n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs} \n",
    "\n",
    "# Load FAISS index\n",
    "vectorstore = FAISS.load_local(\"faiss_index_full\", embeddings,allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1b2c108-8856-4dee-a308-e9f3b826b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: The Allies emerged victorious in World War II, as they were able to defeat both Germany and Japan, leading to the surrender of both countries.\n",
      "Page: 178, Book Title: The OUTBREAK of WORLD WAR II\n",
      "\n",
      "Perspective 2: The United States played a significant role in the victory of the Allies in World War II, especially with the use of atomic bombs on Hiroshima and Nagasaki, which ultimately led to Japan's surrender.\n",
      "Website Link: [https://www.nhb.gov.sg/nationalmuseum/our-exhibitions/exhibition-list/1942-fall-of-singapore]\n",
      "\n",
      "Perspective 3: The Soviet Union's declaration of war in August 1945 also contributed to the defeat of Japan and the eventual end of World War II.\n",
      "Page: 24, Book Title: Back to the Union Jack\n",
      "\n",
      "Discussion Questions:\n",
      "1. How did the use of atomic bombs on Hiroshima and Nagasaki impact the outcome of World War II?\n",
      "2. What role did different countries play in the victory of the Allies in World War II?\n",
      "3. Do you think the end of World War II brought about lasting peace and stability in the Asia Pacific region?\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"Who won in World War Two?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])\n",
    "#print(f\"Referenced sources: {[doc.metadata['source'] for doc in response['context']]}\")\n",
    "# print(\"Referenced sources:\")\n",
    "# for doc in response['context']:\n",
    "#     print(f\"Page {doc.metadata['page']} (Source: {doc.metadata['source']}):\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d00612d7-2094-4f7a-84ba-f2477c97ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referenced sources:\n",
      "Page: 37\n",
      "Source: Sec2\n",
      "\n",
      "Page Content:\n",
      "In spite of its early dominance in Southeast Asia, Japan was eventually \n",
      "defeated in World War II. As the war dragged on, Japan experienced an \n",
      "increasing number of defeats by the Allies, such as in the Battle of Midway. \n",
      "The sea battles in particular helped to stall Japan’s advance throughout \n",
      "Southeast Asia. In 1945, Germany surrendered to the Allies, who then \n",
      "focused their resources on defeating Japan. \n",
      "On 6 and 9 August, the Allies dropped an atomic bomb each on the \n",
      "Japanese cities of Hiroshima and Nagasaki respectively. The estimated total \n",
      "death toll was 214,000. The bombs also caused massive damage in both \n",
      "cities. On 15 August 1945, Emperor Hirohito announced Japan’s surrender to \n",
      "the Allies. On 2 September 1945, the formal surrender of Japan took place\n",
      "\n",
      "Page: 184\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "Why Did War Break Out in the Asia Pacific in 1941?\n",
      "To understand why World War II began and spread to the Asia Pacific, we must look at what happened before \n",
      "that. In the 1920s, the British Empire was the largest in the world, comprising over a quarter of the world’s land \n",
      "area and population. However, in the 1930s, two rising countries, Germany and Japan, began to pose serious \n",
      "threats to the British Empire and to international peace, as illustrated below:\n",
      "The economy was in ruins and there was great hardship.\n",
      "In these circumstances, Adolf Hitler \n",
      "and his Nazi Party rose to power. They wanted to make Germany  \n",
      "a great power again, and hence won \n",
      "the support of many Germans.\n",
      "They rebuilt the German \n",
      "military into something \n",
      "more powerful than before.\n",
      "\n",
      "Page: 184\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "They rebuilt the German \n",
      "military into something \n",
      "more powerful than before.\n",
      "By the late 1930s, it seemed as if Germany \n",
      "was getting ready for another war.* A great power is a very strong country that can dominate or influence other countries.After World War I, Germany lost its status as a great power. *\n",
      "175\n",
      "DID SINGAPORE HAVE TO FALL TO THE JAPANESE IN WORLD WAR II?The RISE of NAZI GERMANYThe RISE of NAZI GERMANY\n",
      "\n",
      "Page: 187\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "also beating more loudly.\n",
      "In 1937, Japan invaded China.\n",
      "To force the Japanese out of China, the \n",
      "United States stopped selling oil to Japan.Japan would not withdraw from China. But it was \n",
      "desperate for oil to keep its industries going.\n",
      "GERMANY\n",
      "FRANCEBELGIUMNETHERLANDSBRITAIN\n",
      "English ChannelNorth Sea\n",
      "By 1940, Germany had defeated France \n",
      "and taken over most of Europe.In September 1939, Germany invaded Poland. To stop Hitler, Britain and \n",
      "France declared war on Germany. Thus, World War II began in Europe.GERMANYPOLAND\n",
      "178\n",
      "The OUTBREAK of WORLD WAR IIThe OUTBREAK of WORLD WAR II\n",
      "\n",
      "Title: World War Ii\n",
      "Source: Roots Website\n",
      "Url: https://www.roots.gov.sg/stories-landing/stories/world-war-ii/story\n",
      "\n",
      "Page Content:\n",
      "Back to the Union Jack\n",
      "As World War Two dragged on, the Japanese began to suffer numerous defeats in other parts of the world.24 Following the atomic bombings of Hiroshima and Nagasaki, and the Soviet Union's declaration of war in August, Japan formally surrendered on 2 September 1945, in Tokyo Bay on the American battleship USS Missouri, bringing an end to the war and their occupation of Singapore.25\n",
      "The Chamber of the Former City Hall, where the Japanese surrendered on 12 September 1945. (c.1945. Image from National Archives of Singapore)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    # Print all metadata keys and values\n",
    "    #print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        # Check if the key is 'title', 'source', or 'url' and handle them accordingly\n",
    "        if key in ['title', 'source', 'url']:\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "        else:\n",
    "            # Print other metadata normally\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Print the page content as well\n",
    "    print(f\"\\nPage Content:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d56d22-78a4-4233-8af3-ebc1b8b863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to query from the vector store\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Given a user's question and any provided filters (student age group, historical timeframe, theme), please:\n",
    "\n",
    "1. Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective.\n",
    "For each perspective, provide citations in page number\n",
    "\n",
    "2. Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "3. If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "4. After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a297e928-f804-4d0b-b87c-4fcc9cc98dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing with similarity search with relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bda2ac27-c337-4ef2-a118-fdc3b0aeadf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minli\\AppData\\Local\\Temp\\ipykernel_44812\\3938583706.py:44: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'title': 'About A Boy The Adventures Of Tintin', 'source': 'Roots Website', 'url': 'https://www.roots.gov.sg/stories-landing/stories/about-a-boy-the-adventures-of-tintin/story'}, page_content=\"â€“ HergÃ©, 1943\\r\\nLast, but not least Herge&ecute; himself\\r\\nThe creator had claimed â€œTintin, c'est moiâ€\\x9d or â€œTintin, it's me.â€\\x9d ... â€œThese are my eyes, my meaning, my lungs, my tripe! ... I believe that I am alone in being able to animate, in the sense of giving a soul.â€\\x9d\\r\\nKnow the cast\\r\\nHergÃ©'s world was occupied by sharply defined characters. They were mostly inspired by real people who were either close to him, historical figures or celebrities at that time. Their portraits and psychological profiles had been meticulously composed, leaving nothing to chance.\\r\\n>>Snowy\"), -0.04180679062283699), (Document(metadata={'title': 'About A Boy The Adventures Of Tintin', 'source': 'Roots Website', 'url': 'https://www.roots.gov.sg/stories-landing/stories/about-a-boy-the-adventures-of-tintin/story'}, page_content=\">>Snowy\\r\\nTintin's constant companion is an extraordinary white wire fox terrier. He is known as â€œMilouâ€\\x9d in French. There are several stories on whom Snowy was named after. One version was that Snowy was named after HergÃ©'s girlfriend Marie-Louise Van Cutsem who was called â€œMalouâ€\\x9d. Another candidate was RenÃ© Milhoux, the best friend of French war and travel photojournalist Robert Sexe, who was himself one of many speculated inspirations of the Tintin character.\\r\\nSnowy appeared right at the start, in the first drawing of the first adventure. He sticks by Tintin through good and bad times. He is faithful and courageous, saving Tintin in numerous adventures. He is known to be Tintin's alter ego and his only confidant at the beginning.\\r\\n>>Captain Archibald Haddock\"), -0.04355419896209867), (Document(metadata={'title': nan, 'source': 'Roots Website', 'url': 'https://www.roots.gov.sg/stories-landing/stories/Jingle-Jungle'}, page_content=\"Lyo's name is an acronym for â€œLion of the Youth Olympicsâ€\\x9d â€“ how typically Singaporean it is to shorten a phrase into a handful of letters! He symbolises youthful energy, passion, and constant striving for excellence. He and his companion Merly are the first Youth Olympic Games mascots â€“ ever! (In 2010, Singapore was the first country to host the Youth Olympic Games.\\r\\nPhotograph of mascots to be featured at the National Day Parade in 2018 (28 July 2018. Courtesy of The Straits Times, Singapore Press Holdings.)\\r\\nSharity, Nila, Captain Green, Water Wally, Teamy, and Singa â€“ how many of these characters do you know? There's more to find out about our mascots â€“ click on Chatbird to find out which one you are most like!\"), -0.10423470263717327), (Document(metadata={'title': 'Are You Game How We Used To Play', 'source': 'Roots Website', 'url': 'https://www.roots.gov.sg/stories-landing/stories/are-you-game-how-we-used-to-play/story'}, page_content=\"Capteh\\r\\nWith acrobatic hurricane-whirl kicks and mesmerising moves inspired by natural predators such as the snake, tiger and leopard, the Shaolin monks are renowned as impressive martial artists. What few would believe is that generations of them had used the humble shuttlecock â€” or capteh as we know it â€” to sharpen and hone their kung fu skills.8\\r\\nIt might seem like child's play to get the shuttlecock off the ground but mastering special moves and keeping it airborne for as long as possible with one's feet is a whole other ball game. An international federation has evolved the game into a competitive sport since 1999, with players battling it out on a court with a net in the middle to challenge their agility, balance and strength.\"), -0.11506939298386887), (Document(metadata={'title': 'Horses For Courses', 'source': 'Roots Website', 'url': 'https://www.roots.gov.sg/stories-landing/stories/horses-for-courses/story'}, page_content=\"Portugal: Lusitano Horses\\r\\nStamp of Lusitano Horses\\r\\nThe Lusitano, named in 1966 after Portugal's ancient Roman name, is closely related to the Spanish-Andalusian horse. Lusitanos have competed in the Olympics and World Equestrian Games. In early times, they were deployed for war but these days they only participate in bloodless bullfighting.\\r\\nUkraine: Straw horse\\r\\nStamp of Straw Horse\\r\\nLocated in a region blessed with rich â€˜black soil', Ukraine has abundant wheat. After the grain is removed, the raw is used to feed animals, for housing, and to make traditional handicrafts.\\r\\nUnited Kingdom: All the Queen's Horses\\r\\nStamp of All the Queen's Horses\"), -0.12289741145149424)]\n",
      "  ranked_results = vector_store.similarity_search_with_relevance_scores(input_question, k=k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, the question \"Who is the best Pokemon?\" is not relevant to Singapore's history and culture. Therefore, I don't have an answer for that question. If you have any other questions related to Singapore's history or culture, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import os\n",
    "\n",
    "def answer_question_from_vector_store(vector_store, input_question, k=5):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective. \n",
    "For each perspective, include a source citation from the context ONLY in one of the following formats:\n",
    "\n",
    "Page Number (if the source is a book or document with specific page references),\n",
    "Website Link (if the source is a digital resource or website),\n",
    "Or both if applicable (e.g., a book citation with a page number and a link to the digital source).\n",
    "Format the answer as follows:\n",
    "\n",
    "Perspective #: [Answer summary]\n",
    "Page: [Page Number], [Source]\n",
    "OR\n",
    "Website Link: [Link to the source]\n",
    "\n",
    "If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "\n",
    "After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture. Each citation should be appropriately linked to the perspective it corresponds to, whether it is a page number, website link, or both.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the query is not relevant to Singapore History, Please reply that you dont know the answer.\n",
    "Only answer questions based on the context.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Function to format documents for input into the prompt\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Retrieve documents using similarity search with relevance scores\n",
    "    ranked_results = vector_store.similarity_search_with_relevance_scores(input_question, k=k)\n",
    "\n",
    "    # Collect relevant documents based on the similarity score\n",
    "    relevant_docs = []\n",
    "    for doc, score in ranked_results:\n",
    "        doc.metadata[\"similarity_score\"] = score  # Add the similarity score to metadata\n",
    "        relevant_docs.append(doc)\n",
    "\n",
    "    formatted_context = format_docs(relevant_docs)\n",
    "    \n",
    "    # Chain the context and question into a format suitable for the prompt\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    # Get the result from the RAG chain\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "\n",
    "    return {\"answer\": result.content, \"context\": relevant_docs}\n",
    "\n",
    "# Test query\n",
    "question = \"Who is the best Pokemon?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a788281-e256-4c6f-9773-782f0cc08868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Some argue that Stamford Raffles was the founder of Singapore because he signed the 1819 Treaty that allowed the British to set up a trading post in the southern part of Singapore. This perspective emphasizes the role of Raffles in establishing British presence in Singapore.\n",
      "Website Link: [National Heritage Board - Singapore History](https://www.nhb.gov.sg/singapore-history)\n",
      "\n",
      "Perspective 2: Others believe that William Farquhar was the founder of Singapore as he played a significant role in building Singapore from scratch. This perspective highlights Farquhar's contributions to the early development of the island.\n",
      "Page: 66, FROM TEMASEK TO SINGAPORE (1299–EARLY 1800 S)\n",
      "\n",
      "Perspective 3: Some may consider John Crawfurd the founder of Singapore because he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the whole island. This viewpoint focuses on Crawfurd's role in shaping the governance of Singapore under British rule.\n",
      "Website Link: [National Heritage Board - Singapore History](https://www.nhb.gov.sg/singapore-history)\n",
      "\n",
      "Discussion Questions:\n",
      "1. How do different historical perspectives on the founding of Singapore influence our understanding of the city's origins?\n",
      "2. In what ways do the contributions of Raffles, Farquhar, and Crawfurd reflect the complexities of colonial history in Singapore?\n",
      "3. Why is it important to consider multiple viewpoints when studying historical events like the founding of Singapore?\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"Who is the founder of Singapore?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec4b5233-b772-4040-ab44-c2869b7cfb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referenced sources:\n",
      "Page: 75\n",
      "Source: Sec1\n",
      "Similarity_score: 0.6222825988968239\n",
      "\n",
      "Page Content:\n",
      "66\n",
      "FROM TEMASEK TO SINGAPORE (1299–EARLY 1800 S)Today, Raffles’ name can be seen all over Singapore: Raffles Place MRT station and Raffles Boulevard, just \n",
      "to give a few examples. This is because Raffles is often recognised as the founder of Singapore. However, \n",
      "not everyone takes this view.\n",
      "Given what you have learnt about the contributions of Raffles, Farquhar and Crawfurd to the development \n",
      "of Singapore, who do you think founded Singapore? Before answering this question, think about what it \n",
      "means to be a founder. What does it mean to found a place?\n",
      "Some maintain that Raffles was the founder as he signed the 1819 Treaty that allowed the British to \n",
      "set up a trading post in the southern part of Singapore. Others argue that Farquhar was the founder of\n",
      "\n",
      "Page: 75\n",
      "Source: Sec1\n",
      "Similarity_score: 0.5974677567522163\n",
      "\n",
      "Page Content:\n",
      "Singapore as he did the work of building Singapore from scratch. Some may even consider Crawfurd the \n",
      "founder as he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the  \n",
      "whole island.\n",
      "Read the sources that follow and conduct your own investigation into who founded Singapore.Who founded Singapore?CASE INVESTIGATION\n",
      "\n",
      "Page: 4\n",
      "Source: Sec1\n",
      "Similarity_score: 0.5841961356246209\n",
      "\n",
      "Page Content:\n",
      "set up a trading post in the southern part of Singapore. Others argue that Farquhar was the founder of \n",
      "Singapore as he did the work of building Singapore from scratch. Some may even consider Crawfurd the \n",
      "founder as he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the  \n",
      "whole island.\n",
      "Read the sources that follow and conduct your own investigation into who founded Singapore.Who founded Singapore?CASE INVESTIGATION\n",
      "SINGAPORE’S DEVELOPMENT AS A PORT CITY UNDER THE BRITISH (1819–1942)In Chapter 2, you learnt that historians often examine the causes of key historical events. This is not always \n",
      "a straightforward matter. Related events do not necessarily take place one after another, and one event\n",
      "\n",
      "Page: 4\n",
      "Source: Sec1\n",
      "Similarity_score: 0.5719815996045272\n",
      "\n",
      "Page Content:\n",
      "historical concepts and explains how \n",
      "they are relevant to the study of history.  \n",
      "Amy Lim\n",
      "Amy Lim\n",
      "66 FROM TEMASEK TO SINGAPORE (1299–EARLY 1800 S)Today, Raffles’ name can be seen all over Singapore: Raffles Place MRT station and Raffles Boulevard, just \n",
      "to give a few examples. This is because Raffles is often recognised as the founder of Singapore. However, \n",
      "not everyone takes this view.\n",
      "Given what you have learnt about the contributions of Raffles, Farquhar and Crawfurd to the development \n",
      "of Singapore, who do you think founded Singapore? Before answering this question, think about what it \n",
      "means to be a founder. What does it mean to found a place?\n",
      "Some maintain that Raffles was the founder as he signed the 1819 Treaty that allowed the British to\n",
      "\n",
      "Title: Founding Of Modern Singapore\n",
      "Source: Roots Website\n",
      "Url: https://www.roots.gov.sg/stories-landing/stories/founding-of-modern-singapore/story\n",
      "Similarity_score: 0.557664273018528\n",
      "\n",
      "Page Content:\n",
      "Sitting on the tip of the Malay Peninsula where sea routes intersect, the island of Singapore had long impressed mariners, traders and rulers for its strategic location, good rivers and great potential.2 The British were similarly enamoured by Singapore some centuries later. Under their charge in the 19th and 20th centuries, the ancient port was revived, attracting a melting pot of immigrants who together with founder Stamford Raffles, and British residents of Singapore William Farquhar and John Crawfurd, helped build a thriving emporium and shape the city we know today.\n",
      "An 1817 portrait of Sir Stamford Raffles. In a bid to prevent the Dutch from monopolising trade, Raffles set about looking for a new trading base in the Malay Peninsula. (Image from the National Museum of Singapore.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    # Print all metadata keys and values\n",
    "    #print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        # Check if the key is 'title', 'source', or 'url' and handle them accordingly\n",
    "        if key in ['title', 'source', 'url']:\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "        else:\n",
    "            # Print other metadata normally\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Print the page content as well\n",
    "    print(f\"\\nPage Content:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb955de-228b-4c86-92bf-5765e4f57265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

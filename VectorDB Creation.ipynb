{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce43c6ad-4b6f-4ec4-a30b-890bca928dbe",
   "metadata": {},
   "source": [
    "Vector Database FAISS/ChromaDB/PineCone?\n",
    "\n",
    "-  Step 1: Load sec1 and sec2 PDFs\n",
    "-  Step 2: Split into Chunks + Tag with Metadata ( Page Number + sec 2/sec1 label)\n",
    "-  Step 3: Combine into VectorDB\n",
    "\n",
    "Each Query Should Return:\n",
    "- Page Number + Sec 2/Sec1 source\n",
    "- UI should be able to filter between sec 1 and sec 2 content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825bc95-0ad0-474e-901b-c8a26a252a2c",
   "metadata": {},
   "source": [
    "langchain==0.3.9\n",
    "langchain-community==0.3.1\n",
    "pypdf==4.2.0\n",
    "python-dotenv\n",
    "faiss-cpu==1.7.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c03f34-2781-4d36-a2b7-c01fe0d8b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if a specific variable is loaded\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f027b-7b05-4403-a358-ebff9a846d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Save the full text from each PDF\n",
    "with open('full_text_from_pdfs.txt', 'w', encoding='utf-8') as full_text_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        full_text_file.write(f\"--- {label} ---\\n\")\n",
    "        for page in pages:\n",
    "            # Write the full text from each page into the file\n",
    "            full_text_file.write(page.page_content + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc4d13-fc53-4227-b928-c4c7b32d044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this part only to reload documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d5f11bd-6674-4b82-8536-674433abb4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Verify the PDF loading process\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            documents.append({\n",
    "                \"text\": page.page_content,\n",
    "                \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "#delete this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e703055d-da06-4128-b1e8-a901f2dee010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Ensure documents are stored as Document objects\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            # Create a Document object with the correct structure\n",
    "            doc = Document(\n",
    "                page_content=page.page_content,\n",
    "                metadata={\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785feec8-0a8b-4dde-b330-fbc1f3443058",
   "metadata": {},
   "source": [
    "## Trying different Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b824-7c59-40f7-8236-5b3f706e55cc",
   "metadata": {},
   "source": [
    "1. Recursive Character\n",
    "2. Paragraph\n",
    "3. Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9483d1-4376-47da-91da-b711d2815679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "# Save the chunks into a file for observation\n",
    "with open('text_chunks_recursive.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            splits = text_splitter.split_text(text)  # Split the text into chunks\n",
    "            for split in splits:\n",
    "                chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write the chunk text into the file\n",
    "                chunk_file.write(f\"--- Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a8fd6a-ab4f-4b47-8b77-866393ef42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Chunking text based on paragraph (assuming each paragraph is separated by a new line)\n",
    "paragraph_chunks = []\n",
    "\n",
    "with open('paragraph_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            # Split based on new lines (assumed paragraph boundary)\n",
    "            splits = re.split(r'\\n\\n+', text)  # Split by double newlines\n",
    "            for split in splits:\n",
    "                paragraph_chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write each paragrapbh into the file\n",
    "                chunk_file.write(f\"--- Paragraph Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bf4e5-f6c7-4eab-8d17-f6b111269ef2",
   "metadata": {},
   "source": [
    "document check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9820dd93-ea61-452c-8fc9-7a3360b526b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 436\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate embeddings for the documents to check if embeddings are created correctly\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m embedding_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Generate embeddings using OpenAI embeddings model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m embeddings_vectors \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39membed_documents(embedding_texts)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Ensure documents list is populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n",
    "# Generate embeddings for the documents to check if embeddings are created correctly\n",
    "embedding_texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "# Generate embeddings using OpenAI embeddings model\n",
    "embeddings_vectors = embeddings.embed_documents(embedding_texts)\n",
    "\n",
    "# Check if embeddings were generated correctly\n",
    "if len(embeddings_vectors) == len(embedding_texts):\n",
    "    print(\"Embeddings generated correctly!\")\n",
    "else:\n",
    "    print(f\"Error: Expected {len(embedding_texts)} embeddings, but got {len(embeddings_vectors)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0978f7ea-ddc4-4144-a106-e5e8f0e6cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the combined documents (articles + PDF pages) to a file\n",
    "with open('textbooks.pkl', 'wb') as f:\n",
    "    pickle.dump(documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486500d2-d435-4048-870d-499315c415c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a228b3ac-3c88-4462-a832-e0c77ed8f1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic_text_chunks.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m chunk_file:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m semantic_chunk \u001b[38;5;129;01min\u001b[39;00m semantic_chunks:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# Write the chunk's content and metadata to the file\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Semantic Chunk from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msemantic_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemantic_chunk\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(semantic_chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Add a blank line between chunks\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'source'"
     ]
    }
   ],
   "source": [
    "#rerun this one for results.\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Load the PDFs and create documents list\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append(Document(page_content=page.page_content, metadata={\"page\": page.metadata[\"page\"], \"source\": label}))\n",
    "\n",
    "# Initialize the OpenAI Embeddings model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for semantic_chunk in semantic_chunks:\n",
    "        # Write the chunk's content and metadata to the file\n",
    "        chunk_file.write(f\"--- Semantic Chunk from {semantic_chunk.metadata['source']} (Page {semantic_chunk.metadata['page']}) ---\\n\")\n",
    "        chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "        chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92c2146b-df36-4d7c-b952-a79c9a5dfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    chunk_index = 0  # Initialize chunk index\n",
    "    for doc in documents:\n",
    "        # Get the number of chunks for the current document\n",
    "        doc_chunks = [chunk for chunk in semantic_chunks if chunk.page_content in doc.page_content]\n",
    "        \n",
    "        # Add each chunk with the corresponding metadata\n",
    "        for semantic_chunk in doc_chunks:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'Unknown')\n",
    "            \n",
    "            # Write the chunk's content and metadata to the file\n",
    "            chunk_file.write(f\"--- Semantic Chunk from {source} (Page {page}) ---\\n\")\n",
    "            chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "            chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "            chunk_index += 1  # Increment the chunk index\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696b21a-c247-442a-a15d-b0de002ecac4",
   "metadata": {},
   "source": [
    "## Vector Database Creation Attempts (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25facdf3-c905-428b-9860-d787ad3df3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully!\n"
     ]
    }
   ],
   "source": [
    "# this code is for only creation from the PDF textbooks\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append({\n",
    "            \"text\": page.page_content,\n",
    "            \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "        })\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    splits = text_splitter.split_text(doc[\"text\"])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Save VectorDB\n",
    "faiss_index.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Vector database created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0a6cd32-5856-4b82-9430-2e2aa58fcca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully!\n"
     ]
    }
   ],
   "source": [
    "# second attempt for combined vectorDB creation\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "csv_file = \"roots_sg_articles_cleaned.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Fill NaN values in 'text' column with an empty string or a default string\n",
    "df['text'] = df['text'].fillna('missing content')\n",
    "\n",
    "# Ensure 'text' column is a string\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Create documents from CSV data\n",
    "articles = []\n",
    "for index, row in df.iterrows():\n",
    "    articles.append(Document(\n",
    "        page_content=row['text'],\n",
    "        metadata={\n",
    "            'title': row['title'],\n",
    "            'source': row['source'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Load the processed documents from the pickle file\n",
    "with open('textbooks.pkl', 'rb') as f:\n",
    "    pdf_documents = pickle.load(f)\n",
    "\n",
    "# # Load PDFs\n",
    "# pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "# documents = []\n",
    "\n",
    "# for label, path in pdf_paths.items():\n",
    "#     loader = PyPDFLoader(path)\n",
    "#     pages = loader.load()\n",
    "#     for page in pages:\n",
    "#         documents.append({\n",
    "#             \"text\": page.page_content,\n",
    "#             \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "#         })\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=100\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "# Combine both article and PDF documents into one list\n",
    "all_documents = articles + pdf_documents\n",
    "\n",
    "# Process the documents (both articles and PDFs)\n",
    "for doc in all_documents:\n",
    "    # Access the content using dot notation, not dict-style indexing\n",
    "    splits = text_splitter.split_text(doc.page_content)  # Access the content using .page_content\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata  # Access the metadata using .metadata\n",
    "        })\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Save VectorDB\n",
    "faiss_index.save_local(\"faiss_index_full\")\n",
    "\n",
    "print(\"Vector database created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fadf7d6-b0aa-4a33-b4af-925f5aa942e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4fafb-6c81-457c-b0a5-3d9c3f576baa",
   "metadata": {},
   "source": [
    "## Testing Response from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a64a161-96e4-46ee-8f9a-c05905c57de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_vector_store(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective. For each perspective, include a source citation in one of the following formats:\n",
    "\n",
    "Page Number (if the source is a book or document with specific page references),\n",
    "Website Link (if the source is a digital resource or website),\n",
    "Or both if applicable (e.g., a book citation with a page number and a link to the digital source).\n",
    "Format the answer as follows:\n",
    "\n",
    "Perspective #: [Answer summary]\n",
    "Page: [Page Number], Book Title: Sec1 or Sec2\n",
    "OR\n",
    "Website Link: [Link to the source]\n",
    "OR\n",
    "Page: [Page Number] | Website Link: [Link to the source]\n",
    "Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "\n",
    "If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "\n",
    "After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture. Each citation should be appropriately linked to the perspective it corresponds to, whether it is a page number, website link, or both.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "    \n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    \n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs} \n",
    "\n",
    "# Load FAISS index\n",
    "vectorstore = FAISS.load_local(\"faiss_index_full\", embeddings,allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1b2c108-8856-4dee-a308-e9f3b826b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: The Allies emerged victorious in World War II, as they were able to defeat both Germany and Japan, leading to the surrender of both countries.\n",
      "Page: 178, Book Title: The OUTBREAK of WORLD WAR II\n",
      "\n",
      "Perspective 2: The United States played a significant role in the victory of the Allies in World War II, especially with the use of atomic bombs on Hiroshima and Nagasaki, which ultimately led to Japan's surrender.\n",
      "Website Link: [https://www.nhb.gov.sg/nationalmuseum/our-exhibitions/exhibition-list/1942-fall-of-singapore]\n",
      "\n",
      "Perspective 3: The Soviet Union's declaration of war in August 1945 also contributed to the defeat of Japan and the eventual end of World War II.\n",
      "Page: 24, Book Title: Back to the Union Jack\n",
      "\n",
      "Discussion Questions:\n",
      "1. How did the use of atomic bombs on Hiroshima and Nagasaki impact the outcome of World War II?\n",
      "2. What role did different countries play in the victory of the Allies in World War II?\n",
      "3. Do you think the end of World War II brought about lasting peace and stability in the Asia Pacific region?\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"Who won in World War Two?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])\n",
    "#print(f\"Referenced sources: {[doc.metadata['source'] for doc in response['context']]}\")\n",
    "# print(\"Referenced sources:\")\n",
    "# for doc in response['context']:\n",
    "#     print(f\"Page {doc.metadata['page']} (Source: {doc.metadata['source']}):\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d00612d7-2094-4f7a-84ba-f2477c97ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referenced sources:\n",
      "Page: 37\n",
      "Source: Sec2\n",
      "\n",
      "Page Content:\n",
      "In spite of its early dominance in Southeast Asia, Japan was eventually \n",
      "defeated in World War II. As the war dragged on, Japan experienced an \n",
      "increasing number of defeats by the Allies, such as in the Battle of Midway. \n",
      "The sea battles in particular helped to stall Japan’s advance throughout \n",
      "Southeast Asia. In 1945, Germany surrendered to the Allies, who then \n",
      "focused their resources on defeating Japan. \n",
      "On 6 and 9 August, the Allies dropped an atomic bomb each on the \n",
      "Japanese cities of Hiroshima and Nagasaki respectively. The estimated total \n",
      "death toll was 214,000. The bombs also caused massive damage in both \n",
      "cities. On 15 August 1945, Emperor Hirohito announced Japan’s surrender to \n",
      "the Allies. On 2 September 1945, the formal surrender of Japan took place\n",
      "\n",
      "Page: 184\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "Why Did War Break Out in the Asia Pacific in 1941?\n",
      "To understand why World War II began and spread to the Asia Pacific, we must look at what happened before \n",
      "that. In the 1920s, the British Empire was the largest in the world, comprising over a quarter of the world’s land \n",
      "area and population. However, in the 1930s, two rising countries, Germany and Japan, began to pose serious \n",
      "threats to the British Empire and to international peace, as illustrated below:\n",
      "The economy was in ruins and there was great hardship.\n",
      "In these circumstances, Adolf Hitler \n",
      "and his Nazi Party rose to power. They wanted to make Germany  \n",
      "a great power again, and hence won \n",
      "the support of many Germans.\n",
      "They rebuilt the German \n",
      "military into something \n",
      "more powerful than before.\n",
      "\n",
      "Page: 184\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "They rebuilt the German \n",
      "military into something \n",
      "more powerful than before.\n",
      "By the late 1930s, it seemed as if Germany \n",
      "was getting ready for another war.* A great power is a very strong country that can dominate or influence other countries.After World War I, Germany lost its status as a great power. *\n",
      "175\n",
      "DID SINGAPORE HAVE TO FALL TO THE JAPANESE IN WORLD WAR II?The RISE of NAZI GERMANYThe RISE of NAZI GERMANY\n",
      "\n",
      "Page: 187\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "also beating more loudly.\n",
      "In 1937, Japan invaded China.\n",
      "To force the Japanese out of China, the \n",
      "United States stopped selling oil to Japan.Japan would not withdraw from China. But it was \n",
      "desperate for oil to keep its industries going.\n",
      "GERMANY\n",
      "FRANCEBELGIUMNETHERLANDSBRITAIN\n",
      "English ChannelNorth Sea\n",
      "By 1940, Germany had defeated France \n",
      "and taken over most of Europe.In September 1939, Germany invaded Poland. To stop Hitler, Britain and \n",
      "France declared war on Germany. Thus, World War II began in Europe.GERMANYPOLAND\n",
      "178\n",
      "The OUTBREAK of WORLD WAR IIThe OUTBREAK of WORLD WAR II\n",
      "\n",
      "Title: World War Ii\n",
      "Source: Roots Website\n",
      "Url: https://www.roots.gov.sg/stories-landing/stories/world-war-ii/story\n",
      "\n",
      "Page Content:\n",
      "Back to the Union Jack\n",
      "As World War Two dragged on, the Japanese began to suffer numerous defeats in other parts of the world.24 Following the atomic bombings of Hiroshima and Nagasaki, and the Soviet Union's declaration of war in August, Japan formally surrendered on 2 September 1945, in Tokyo Bay on the American battleship USS Missouri, bringing an end to the war and their occupation of Singapore.25\n",
      "The Chamber of the Former City Hall, where the Japanese surrendered on 12 September 1945. (c.1945. Image from National Archives of Singapore)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    # Print all metadata keys and values\n",
    "    #print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        # Check if the key is 'title', 'source', or 'url' and handle them accordingly\n",
    "        if key in ['title', 'source', 'url']:\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "        else:\n",
    "            # Print other metadata normally\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Print the page content as well\n",
    "    print(f\"\\nPage Content:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d56d22-78a4-4233-8af3-ebc1b8b863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to query from the vector store\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Given a user's question and any provided filters (student age group, historical timeframe, theme), please:\n",
    "\n",
    "1. Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective.\n",
    "For each perspective, provide citations in page number\n",
    "\n",
    "2. Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "3. If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "4. After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a297e928-f804-4d0b-b87c-4fcc9cc98dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing with similarity search with relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bda2ac27-c337-4ef2-a118-fdc3b0aeadf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Raffles as the Founder\n",
      "Source: Singapore's History: Questions and Themes, page 66\n",
      "Summary: Some argue that Raffles is the founder of Singapore because he signed the 1819 Treaty that allowed the British to set up a trading post in the southern part of Singapore.\n",
      "Direct Quote: \"Some maintain that Raffles was the founder as he signed the 1819 Treaty that allowed the British to set up a trading post in the southern part of Singapore.\"\n",
      "\n",
      "Perspective 2: Farquhar as the Founder\n",
      "Source: Singapore's History: Questions and Themes, page 66\n",
      "Summary: Others argue that Farquhar is the founder of Singapore as he did the work of building Singapore from scratch.\n",
      "Direct Quote: \"Others argue that Farquhar was the founder of Singapore as he did the work of building Singapore from scratch.\"\n",
      "\n",
      "Perspective 3: Crawfurd as the Founder\n",
      "Source: Singapore's History: Questions and Themes, page 66\n",
      "Summary: Some may consider Crawfurd the founder of Singapore as he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the whole island.\n",
      "Direct Quote: \"Some may even consider Crawfurd the founder as he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the whole island.\"\n",
      "\n",
      "Discussion Questions:\n",
      "1. How do different historical perspectives shape our understanding of who founded Singapore?\n",
      "2. What criteria should be used to determine who is considered the founder of a place?\n",
      "3. How do the contributions of Raffles, Farquhar, and Crawfurd reflect the complexities of colonial history in Singapore?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific user warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Relevance scores must be between 0 and 1\")\n",
    "\n",
    "\n",
    "\n",
    "def answer_question_from_vector_store(vector_store, input_question, k=5):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions.\n",
    "\n",
    "Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective. \n",
    "\n",
    "Please format your response using the following structure:\n",
    "\n",
    "Perspective 1 : (Brief summary of perspective)\n",
    "Source: (cite the specific page number or web link and document source)\n",
    "Summary: (2-3 sentences using ONLY information from the cited source)\n",
    "Direct Quote: (exact quote from the source that supports this perspective)\n",
    "\n",
    "Perspective 2 : (Brief summary of perspective)\n",
    "Source: (cite the specific page number and document source)\n",
    "Summary: (2-3 sentences using ONLY information from the cited source)\n",
    "Direct Quote: (exact quote from the source that supports this perspective)\n",
    "\n",
    "[Additional Perspectives if supported by context...]\n",
    "\n",
    "[Discussion Questions]\n",
    "(Only include questions that can be answered using the provided context)\n",
    "1. (question that encourages critical thinking)\n",
    "2. (question that encourages critical thinking)\n",
    "3. (question that encourages critical thinking)\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the query is not relevant to Singapore History, Please reply that you dont know the answer.Only answer questions based on the context, do not hallucinate.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Function to format documents for input into the prompt\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Retrieve documents using similarity search with relevance scores\n",
    "    ranked_results = vector_store.similarity_search_with_relevance_scores(input_question, k=k)\n",
    "\n",
    "    # # Collect relevant documents based on the similarity score\n",
    "    # relevant_docs = []\n",
    "    # for doc, score in ranked_results:\n",
    "    #     doc.metadata[\"similarity_score\"] = score  # Add the similarity score to metadata\n",
    "    #     relevant_docs.append(doc)\n",
    "\n",
    "    # formatted_context = format_docs(relevant_docs)\n",
    "\n",
    "        # Collect relevant documents based on the similarity score (filter out negative scores)\n",
    "    relevant_docs = []\n",
    "    for doc, score in ranked_results:\n",
    "        if 0.2 <= score <= 1:  # Only accept valid scores between 0 and 1\n",
    "            doc.metadata[\"similarity_score\"] = score  # Add the similarity score to metadata\n",
    "            relevant_docs.append(doc)\n",
    "\n",
    "    formatted_context = format_docs(relevant_docs)\n",
    "    \n",
    "    # Chain the context and question into a format suitable for the prompt\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    # Get the result from the RAG chain\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "\n",
    "    return {\"answer\": result.content, \"context\": relevant_docs}\n",
    "\n",
    "# Test query\n",
    "question = \"Who is the founder of Singapore ?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a788281-e256-4c6f-9773-782f0cc08868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Singapore became an independent country following its exit from Malaysia on 9 August 1965. Thrust into nationhood amid a volatile regional climate plagued by conflict (Indonesia was waging an undeclared war, the Konfrontasi, to oppose the formation of Malaysia), Singapore's founding generation of leaders focused on cohesiveness to ensure its survival.\n",
      "Source: Adapted from The Straits Times, 10 August 1965\n",
      "Summary: Singapore's separation from Malaysia was a result of the volatile regional climate and the need for survival as an independent nation.\n",
      "Direct Quote: \"Singapore's founding generation of leaders focused on cohesiveness to ensure its survival.\"\n",
      "\n",
      "Perspective 2: Singapore's separation from Malaysia on 9 August 1965 was greeted with mixed responses. Many raised concerns over Singapore’s ability to survive on its own.\n",
      "Source: Headline in The Straits Times, 10 August 1965\n",
      "Summary: The separation of Singapore from Malaysia was met with skepticism about Singapore's ability to survive independently.\n",
      "Direct Quote: \"Many raised concerns over Singapore’s ability to survive on its own.\"\n",
      "\n",
      "Discussion Questions:\n",
      "1. How did the volatile regional climate, including the Konfrontasi conflict, impact Singapore's decision to become independent?\n",
      "2. What steps did Singapore's founding generation of leaders take to ensure the survival of the newly independent nation?\n",
      "3. How did the mixed responses to Singapore's separation from Malaysia shape the early challenges faced by the independent nation?\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"What caused Singapore to become Independent?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e00e9-dcee-4c5c-b47f-f75031c836a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec4b5233-b772-4040-ab44-c2869b7cfb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referenced sources:\n",
      "Page: 122\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6950448127989101\n",
      "\n",
      "Page Content:\n",
      "Within a few weeks, the Independence of Singapore Agreement was \n",
      "signed and Singapore was no longer a part of Malaysia. On 9 August 1965, \n",
      "an emotional Prime Minister Lee announced Singapore’s separation from \n",
      "the Federation in a live televised press conference:\n",
      "Everytime we look back to the moment we signed this \n",
      "document it is for us a moment of anguish. … All my \n",
      "life, my whole adult life, I have believed in merger and \n",
      "the unity of the two territories. We are connected \n",
      "by geography, economics and ties of kinship. It broke \n",
      "everything we stood for.\n",
      "– Adapted from The Straits Times, 10 August 1965\n",
      "113\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n",
      "Page: 112\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6340082314741913\n",
      "\n",
      "Page Content:\n",
      "declare, on behalf of the people of \n",
      "Singapore, that as from today, the \n",
      "16th day of September, 1963, Singapore \n",
      "shall be forever a part of the sovereign \n",
      "democratic and independent State of \n",
      "Malaysia, founded upon the principles \n",
      "of liberty and justice and ever seeking \n",
      "the welfare and happiness of her \n",
      "people in a more just and more \n",
      "equal society.\n",
      " Prime Minister Lee Kuan Yew speaking \n",
      "at the Malaysia Day celebrations at \n",
      "City Hall, 16 September 1963\n",
      "103\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n",
      "Title: We Built A Nation\n",
      "Source: Roots Website\n",
      "Url: https://www.roots.gov.sg/stories-landing/stories/We-built-a-nation/We-Built-a-Nation\n",
      "Similarity_score: 0.6171269856527288\n",
      "\n",
      "Page Content:\n",
      "Singapore became an independent country following its exit from Malaysia on 9 August 1965. Thrust into nationhood amid a volatile regional climate plagued by conflict (Indonesia was waging an undeclared war, the Konfrontasi, to oppose the formation of Malaysia), Singapore's founding generation of leaders focused on cohesiveness to ensure its survival. In the words of Singapore's first President Mr Yusof Ishak â€œIf we are to remain a cohesive people, we must concentrate on the factors which bind us together, and not those which will divide us.\"\n",
      "\n",
      "Page: 124\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6091323185460913\n",
      "\n",
      "Page Content:\n",
      "Singapore’s separation from Malaysia on 9 August 1965 was greeted with \n",
      "mixed responses. Many raised concerns over Singapore’s ability to survive \n",
      "on its own. In the next two chapters, you will learn about the steps \n",
      "Singapore took to cope with the challenges it faced as an independent \n",
      "nation and what it did to ensure its survival in the long term.\n",
      " Headline in The Straits Times, 10 August 1965, proclaiming the separation of Singapore from MalaysiaConclusion\n",
      "115\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n",
      "Page: 100\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6026631144744572\n",
      "\n",
      "Page Content:\n",
      "Federation of Malaya Federation of Malaya if \n",
      "merged with SingaporeFederation of Malaya if \n",
      "merged with Singapore, \n",
      "North Borneo and Sarawak02\n",
      "134Population (million)ChineseMalay3.1m\n",
      "2.3m3.4m3.6m4.0m\n",
      "3.7m\n",
      "91\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    # Print all metadata keys and values\n",
    "    #print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        # Check if the key is 'title', 'source', or 'url' and handle them accordingly\n",
    "        if key in ['title', 'source', 'url']:\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "        else:\n",
    "            # Print other metadata normally\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Print the page content as well\n",
    "    print(f\"\\nPage Content:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "efb955de-228b-4c86-92bf-5765e4f57265",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Match to each perspective to top source by Cosine Similarity...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1b4f9-9cdc-4343-b7ef-c2588935102a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce43c6ad-4b6f-4ec4-a30b-890bca928dbe",
   "metadata": {},
   "source": [
    "Vector Database FAISS/ChromaDB/PineCone?\n",
    "\n",
    "-  Step 1: Load sec1 and sec2 PDFs\n",
    "-  Step 2: Split into Chunks + Tag with Metadata ( Page Number + sec 2/sec1 label)\n",
    "-  Step 3: Combine into VectorDB\n",
    "\n",
    "Each Query Should Return:\n",
    "- Page Number + Sec 2/Sec1 source\n",
    "- UI should be able to filter between sec 1 and sec 2 content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825bc95-0ad0-474e-901b-c8a26a252a2c",
   "metadata": {},
   "source": [
    "langchain==0.3.9\n",
    "langchain-community==0.3.1\n",
    "pypdf==4.2.0\n",
    "python-dotenv\n",
    "faiss-cpu==1.7.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f230e6-96bb-4159-b8b4-787d70cec0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f027b-7b05-4403-a358-ebff9a846d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Save the full text from each PDF\n",
    "with open('full_text_from_pdfs.txt', 'w', encoding='utf-8') as full_text_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        full_text_file.write(f\"--- {label} ---\\n\")\n",
    "        for page in pages:\n",
    "            # Write the full text from each page into the file\n",
    "            full_text_file.write(page.page_content + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc4d13-fc53-4227-b928-c4c7b32d044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this part only to reload documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d5f11bd-6674-4b82-8536-674433abb4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "\n",
    "# Verify the PDF loading process\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            documents.append({\n",
    "                \"text\": page.page_content,\n",
    "                \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "#delete this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e703055d-da06-4128-b1e8-a901f2dee010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 220 pages from Sec1\n",
      "Successfully loaded 216 pages from Sec2\n",
      "Documents loaded: 436\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Ensure documents are stored as Document objects\n",
    "for label, path in pdf_paths.items():\n",
    "    try:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from {label}\")\n",
    "        \n",
    "        for page in pages:\n",
    "            # Create a Document object with the correct structure\n",
    "            doc = Document(\n",
    "                page_content=page.page_content,\n",
    "                metadata={\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "\n",
    "# Check if documents are populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty after loading PDFs!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785feec8-0a8b-4dde-b330-fbc1f3443058",
   "metadata": {},
   "source": [
    "## Trying different Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b824-7c59-40f7-8236-5b3f706e55cc",
   "metadata": {},
   "source": [
    "1. Recursive Character\n",
    "2. Paragraph\n",
    "3. Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9483d1-4376-47da-91da-b711d2815679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "# Save the chunks into a file for observation\n",
    "with open('text_chunks_recursive.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            splits = text_splitter.split_text(text)  # Split the text into chunks\n",
    "            for split in splits:\n",
    "                chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write the chunk text into the file\n",
    "                chunk_file.write(f\"--- Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a8fd6a-ab4f-4b47-8b77-866393ef42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Chunking text based on paragraph (assuming each paragraph is separated by a new line)\n",
    "paragraph_chunks = []\n",
    "\n",
    "with open('paragraph_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for label, path in pdf_paths.items():\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            text = page.page_content\n",
    "            # Split based on new lines (assumed paragraph boundary)\n",
    "            splits = re.split(r'\\n\\n+', text)  # Split by double newlines\n",
    "            for split in splits:\n",
    "                paragraph_chunks.append({\n",
    "                    \"text\": split,\n",
    "                    \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "                })\n",
    "                # Write each paragrapbh into the file\n",
    "                chunk_file.write(f\"--- Paragraph Chunk from {label} (Page {page.metadata['page']}) ---\\n\")\n",
    "                chunk_file.write(split + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bf4e5-f6c7-4eab-8d17-f6b111269ef2",
   "metadata": {},
   "source": [
    "document check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9820dd93-ea61-452c-8fc9-7a3360b526b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 436\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate embeddings for the documents to check if embeddings are created correctly\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m embedding_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Generate embeddings using OpenAI embeddings model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m embeddings_vectors \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39membed_documents(embedding_texts)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Ensure documents list is populated\n",
    "if not documents:\n",
    "    print(\"Documents list is empty!\")\n",
    "else:\n",
    "    print(f\"Documents loaded: {len(documents)}\")\n",
    "\n",
    "# Generate embeddings for the documents to check if embeddings are created correctly\n",
    "embedding_texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "# Generate embeddings using OpenAI embeddings model\n",
    "embeddings_vectors = embeddings.embed_documents(embedding_texts)\n",
    "\n",
    "# Check if embeddings were generated correctly\n",
    "if len(embeddings_vectors) == len(embedding_texts):\n",
    "    print(\"Embeddings generated correctly!\")\n",
    "else:\n",
    "    print(f\"Error: Expected {len(embedding_texts)} embeddings, but got {len(embeddings_vectors)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0978f7ea-ddc4-4144-a106-e5e8f0e6cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the combined documents (articles + PDF pages) to a file\n",
    "with open('textbooks.pkl', 'wb') as f:\n",
    "    pickle.dump(documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486500d2-d435-4048-870d-499315c415c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a228b3ac-3c88-4462-a832-e0c77ed8f1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic_text_chunks.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m chunk_file:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m semantic_chunk \u001b[38;5;129;01min\u001b[39;00m semantic_chunks:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# Write the chunk's content and metadata to the file\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Semantic Chunk from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msemantic_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemantic_chunk\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(semantic_chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m         chunk_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Add a blank line between chunks\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'source'"
     ]
    }
   ],
   "source": [
    "#rerun this one for results.\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "# Load the PDFs and create documents list\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append(Document(page_content=page.page_content, metadata={\"page\": page.metadata[\"page\"], \"source\": label}))\n",
    "\n",
    "# Initialize the OpenAI Embeddings model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    for semantic_chunk in semantic_chunks:\n",
    "        # Write the chunk's content and metadata to the file\n",
    "        chunk_file.write(f\"--- Semantic Chunk from {semantic_chunk.metadata['source']} (Page {semantic_chunk.metadata['page']}) ---\\n\")\n",
    "        chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "        chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92c2146b-df36-4d7c-b952-a79c9a5dfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the SemanticChunker with the model\n",
    "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# Chunk the documents semantically\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# Save semantic chunks to a file for observation\n",
    "with open('semantic_text_chunks.txt', 'w', encoding='utf-8') as chunk_file:\n",
    "    chunk_index = 0  # Initialize chunk index\n",
    "    for doc in documents:\n",
    "        # Get the number of chunks for the current document\n",
    "        doc_chunks = [chunk for chunk in semantic_chunks if chunk.page_content in doc.page_content]\n",
    "        \n",
    "        # Add each chunk with the corresponding metadata\n",
    "        for semantic_chunk in doc_chunks:\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'Unknown')\n",
    "            \n",
    "            # Write the chunk's content and metadata to the file\n",
    "            chunk_file.write(f\"--- Semantic Chunk from {source} (Page {page}) ---\\n\")\n",
    "            chunk_file.write(semantic_chunk.page_content + \"\\n\")\n",
    "            chunk_file.write(\"\\n\")  # Add a blank line between chunks\n",
    "\n",
    "            chunk_index += 1  # Increment the chunk index\n",
    "\n",
    "# Optional: Print a chunk if it contains a specific phrase, e.g., \"Effect of Pre-training Tasks\"\n",
    "for semantic_chunk in semantic_chunks:\n",
    "    if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
    "        print(f\"--- Found Chunk ---\\n{semantic_chunk.page_content}\")\n",
    "        print(f\"Length: {len(semantic_chunk.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696b21a-c247-442a-a15d-b0de002ecac4",
   "metadata": {},
   "source": [
    "## Vector Database Creation Attempts (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25facdf3-c905-428b-9860-d787ad3df3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully!\n"
     ]
    }
   ],
   "source": [
    "# this code is for only creation from the PDF textbooks\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "# Load PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "documents = []\n",
    "\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        documents.append({\n",
    "            \"text\": page.page_content,\n",
    "            \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "        })\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    splits = text_splitter.split_text(doc[\"text\"])\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Save VectorDB\n",
    "faiss_index.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Vector database created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0a6cd32-5856-4b82-9430-2e2aa58fcca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully!\n"
     ]
    }
   ],
   "source": [
    "# second attempt for combined vectorDB creation\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "csv_file = \"roots_sg_articles_cleaned.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Fill NaN values in 'text' column with an empty string or a default string\n",
    "df['text'] = df['text'].fillna('missing content')\n",
    "\n",
    "# Ensure 'text' column is a string\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Create documents from CSV data\n",
    "articles = []\n",
    "for index, row in df.iterrows():\n",
    "    articles.append(Document(\n",
    "        page_content=row['text'],\n",
    "        metadata={\n",
    "            'title': row['title'],\n",
    "            'source': row['source'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Load the processed documents from the pickle file\n",
    "with open('textbooks.pkl', 'rb') as f:\n",
    "    pdf_documents = pickle.load(f)\n",
    "\n",
    "# # Load PDFs\n",
    "# pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "# documents = []\n",
    "\n",
    "# for label, path in pdf_paths.items():\n",
    "#     loader = PyPDFLoader(path)\n",
    "#     pages = loader.load()\n",
    "#     for page in pages:\n",
    "#         documents.append({\n",
    "#             \"text\": page.page_content,\n",
    "#             \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "#         })\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=100\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "# Combine both article and PDF documents into one list\n",
    "all_documents = articles + pdf_documents\n",
    "\n",
    "# Process the documents (both articles and PDFs)\n",
    "for doc in all_documents:\n",
    "    # Access the content using dot notation, not dict-style indexing\n",
    "    splits = text_splitter.split_text(doc.page_content)  # Access the content using .page_content\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata  # Access the metadata using .metadata\n",
    "        })\n",
    "\n",
    "# Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Save VectorDB\n",
    "faiss_index.save_local(\"faiss_index_full\")\n",
    "\n",
    "print(\"Vector database created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fadf7d6-b0aa-4a33-b4af-925f5aa942e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minli\\AppData\\Local\\Temp\\ipykernel_37808\\1649153934.py:79: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector database created successfully! Total documents stored: 39513\n"
     ]
    }
   ],
   "source": [
    "# include infopedia\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Function to sanitize metadata (ensures valid data types for FAISS)\n",
    "def sanitize_metadata(metadata):\n",
    "    return {k: (v if isinstance(v, (str, int, float, bool)) else \"Unknown\") for k, v in metadata.items()}\n",
    "\n",
    "# Step 1: Load Articles from CSV\n",
    "csv_file = \"data/roots_sg_articles_cleaned.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Fill NaN values and ensure text is string type\n",
    "df['text'] = df['text'].fillna('missing content').astype(str)\n",
    "\n",
    "# Convert CSV data into LangChain Document objects\n",
    "articles = []\n",
    "for _, row in df.iterrows():\n",
    "    articles.append(Document(\n",
    "        page_content=row['text'],\n",
    "        metadata=sanitize_metadata({\n",
    "            'title': row['title'],\n",
    "            'source': row['source'],\n",
    "            'url': row['url']\n",
    "        })\n",
    "    ))\n",
    "\n",
    "# Step 2: Load Processed PDF Documents from Pickle File\n",
    "with open('data/textbooks.pkl', 'rb') as f:\n",
    "    pdf_documents = pickle.load(f)\n",
    "\n",
    "# Ensure PDF documents have sanitized metadata\n",
    "pdf_documents = [\n",
    "    Document(page_content=doc.page_content, metadata=sanitize_metadata(doc.metadata))\n",
    "    for doc in pdf_documents\n",
    "]\n",
    "\n",
    "# Step 3: Load Infopedia Articles from Pickle File\n",
    "with open(\"data/infopedia.pickle\", \"rb\") as f:\n",
    "    infopedia_data = pickle.load(f)\n",
    "\n",
    "# Convert Infopedia data into LangChain Document objects\n",
    "infopedia_articles = []\n",
    "for title, details in infopedia_data.items():\n",
    "    infopedia_articles.append(Document(\n",
    "        page_content=details[\"content\"],\n",
    "        metadata=sanitize_metadata({\n",
    "            'title': title,\n",
    "            'source': details.get('source', 'Unknown'),\n",
    "            'url': details.get('url', 'No URL'),\n",
    "            'last_update_date': details.get('last_update_date', 'Unknown')\n",
    "        })\n",
    "    ))\n",
    "\n",
    "# Combine all document sources (CSV, PDF, Infopedia)\n",
    "all_documents = articles + pdf_documents + infopedia_articles\n",
    "\n",
    "# Step 4: Chunk the Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=100\n",
    ")\n",
    "chunks = []\n",
    "\n",
    "for doc in all_documents:\n",
    "    splits = text_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "\n",
    "# Step 5: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Step 6: Create FAISS Vector Store\n",
    "faiss_index = FAISS.from_texts(\n",
    "    [chunk[\"text\"] for chunk in chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in chunks]\n",
    ")\n",
    "\n",
    "# Step 7: Save the FAISS Vector Store\n",
    "faiss_index.save_local(\"faiss_index_infopedia\")\n",
    "\n",
    "print(f\"✅ Vector database created successfully! Total documents stored: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4fafb-6c81-457c-b0a5-3d9c3f576baa",
   "metadata": {},
   "source": [
    "## Testing Response from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a64a161-96e4-46ee-8f9a-c05905c57de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_vector_store(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective. For each perspective, include a source citation in one of the following formats:\n",
    "\n",
    "Page Number (if the source is a book or document with specific page references),\n",
    "Website Link (if the source is a digital resource or website),\n",
    "Or both if applicable (e.g., a book citation with a page number and a link to the digital source).\n",
    "Format the answer as follows:\n",
    "\n",
    "Perspective #: [Answer summary]\n",
    "Page: [Page Number], Book Title: Sec1 or Sec2\n",
    "OR\n",
    "Website Link: [Link to the source]\n",
    "OR\n",
    "Page: [Page Number] | Website Link: [Link to the source]\n",
    "Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "\n",
    "If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "\n",
    "After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture. Each citation should be appropriately linked to the perspective it corresponds to, whether it is a page number, website link, or both.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "    \n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    \n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs} \n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load FAISS index\n",
    "vectorstore = FAISS.load_local(\"faiss_index_infopedia\", embeddings,allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1b2c108-8856-4dee-a308-e9f3b826b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Raffles as a Visionary Leader\n",
      "Summary: Raffles had grand ambitions for Singapore, including education, urban planning, and law enforcement. He aimed to address issues like slavery and piracy while promoting harmony among immigrant groups.\n",
      "Page: 290, Book Title: Memoir of the Life and Public Services of Sir Thomas Stamford Raffles\n",
      "\n",
      "Perspective 2: Farquhar's Pragmatic Approach\n",
      "Summary: Farquhar, who took over leadership in Raffles' absence, focused on ensuring the port's survival with the available resources, even if it meant deviating from Raffles' specific plans.\n",
      "Page: 63-64, Book Title: An Anecdotal History of Old Times in Singapore\n",
      "\n",
      "Perspective 3: Raffles' Discontent with Farquhar's Leadership\n",
      "Summary: Upon his return to Singapore, Raffles was displeased with the neglect of his plans and the tolerance of local vices like opium and slave trading. This led to Farquhar's dismissal and Raffles taking steps to clean up the colony.\n",
      "Page: 68, Book Title: Memoir of the Life and Public Services of Sir Thomas Stamford Raffles\n",
      "\n",
      "Discussion Questions:\n",
      "1. How did Raffles' idealistic vision for Singapore compare to the practical realities faced by Farquhar during his leadership?\n",
      "2. What factors might have influenced Raffles' decision to dismiss Farquhar and take a more hands-on approach upon his return to Singapore?\n",
      "3. In what ways did the differing perspectives of Raffles and Farquhar impact the development of Singapore during this period?\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"How would Raffles’ actions have been viewed by the different parties??\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])\n",
    "#print(f\"Referenced sources: {[doc.metadata['source'] for doc in response['context']]}\")\n",
    "# print(\"Referenced sources:\")\n",
    "# for doc in response['context']:\n",
    "#     print(f\"Page {doc.metadata['page']} (Source: {doc.metadata['source']}):\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d00612d7-2094-4f7a-84ba-f2477c97ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referenced sources:\n",
      "Page: 66\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "How would Raffles’ \n",
      "actions have been \n",
      "viewed by the  \n",
      "different parties?Think!\n",
      " Pages of the 1819 Treaty between Sir Stamford Raffles, \n",
      "Sultan Hussein and Temenggong Abdul Rahman\n",
      "\n",
      "Title: The First Resident\n",
      "Source: Roots Website\n",
      "Url: https://www.roots.gov.sg/stories-landing/stories/the-first-resident/the-first-resident\n",
      "\n",
      "Page Content:\n",
      "Raffles held on to lofty goals for Singapore such as education, urban planning, and the enforcement of law and order, while aiming to resolve issues such as slavery, piracy and friction among immigrant groups. However, the reality was far from ideal as Farquhar took a more permissive approach in his leadership. He made do with the resources he had to ensure the port's survival â€“ even if the plans deviated from Raffles' specific directions.\n",
      "Farquhar's dismissal from Singapore\n",
      "When Raffles returned to Singapore in 1823, he was furious to find out that his plans had been neglected, and that local vices such as trading of opium and slaves were being tolerated. Farquhar was effectively dismissed and he was succeeded as British Resident by Dr John Crawfurd.\n",
      "\n",
      "Page: 66\n",
      "Source: Sec1\n",
      "\n",
      "Page Content:\n",
      "power in the region.\n",
      "On 6 February 1819, Raffles recognised Tengku Hussein as the \n",
      "rightful Sultan. He signed an agreement with Sultan Hussein and the \n",
      "Temenggong which allowed the British to build a trading post in the \n",
      "southern part of Singapore island. In return, the Temenggong and \n",
      "Sultan Hussein would receive an annual payment as compensation. \n",
      "The British also agreed to give protection and support to the Sultan \n",
      "and Temenggong if they agreed not to make any treaties with other \n",
      "Western powers. After signing the treaty, Raffles made Farquhar the \n",
      "first Resident * of Singapore and left for Penang the next day.\n",
      "* The title of Resident is given to the British officer in charge of a British settlement.\n",
      "How would Raffles’ \n",
      "actions have been \n",
      "viewed by the  \n",
      "different parties?Think!\n",
      "\n",
      "Title: Becoming A Cosmopolitan Port City\n",
      "Source: Roots Website\n",
      "Url: https://www.roots.gov.sg/stories-landing/stories/becoming-a-cosmopolitan-port-city/becoming-a-cosmopolitan-port-city\n",
      "\n",
      "Page Content:\n",
      "Raffles was dissatisfied with Farquhar's leadership and immediately set into motion plans to clean up the colony. In 1822, Raffles formed a Town Committee with the help of the colony's engineer and land surveyor, Lieutenant Philip Jackson, to tackle the situation. From his personal observations of other colonial towns, such as Georgetown in Penang, he knew that a formal town plan had led to the successfully integration of Indian and Chinese immigrants. This was the solution he settled on, as it prevented groups from developing separately and creating haphazard settlements.\n",
      "The Jackson Plan\n",
      "\n",
      "Title: Gambling farms in the 19th century\n",
      "Source: Singapore Infopedia\n",
      "Url: https://www.nlb.gov.sg/main/article-detail?cmsuuid=a120b925-44c0-49d8-af19-9f475c536f64\n",
      "Last_update_date: July 2020\n",
      "\n",
      "Page Content:\n",
      "no. RSING 336.200959 RIS)3. Sophia Raffles, Memoir of the Life and Public Services of Sir Thomas Stamford Raffles (Singapore: Oxford University Press, 1991), 290, 297–98. (Call no. RSING 959.57021092 RAF-[HIS])4. T. S. Raffles, “Minute By the Lieutenant Governor,” in Sophia Raffles, Memoir of the Life and Public Services of Sir Thomas Stamford Raffles (Singapore: Oxford University Press, 1991), 68. (Call no. RSING 959.57021092 RAF-[HIS])5. Charles Burton Buckley, An Anecdotal History of Old Times in Singapore (Singapore: Oxford University Press, 1984), 63–64 (Call no. RSING 959.57 BUC-[HIS]); Maurice Freedman, Colonial Law and Chinese Society (London: Royal Anthropological Institute of Great Britain and Ireland, 1952), 97. (Call no. RDTYS 301.42 FRE)   6. T. S. Raffles, “Regulation, No.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    # Print all metadata keys and values\n",
    "    #print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        # Check if the key is 'title', 'source', or 'url' and handle them accordingly\n",
    "        if key in ['title', 'source', 'url']:\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "        else:\n",
    "            # Print other metadata normally\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Print the page content as well\n",
    "    print(f\"\\nPage Content:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2d56d22-78a4-4233-8af3-ebc1b8b863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to query from the vector store\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Given a user's question and any provided filters (student age group, historical timeframe, theme), please:\n",
    "\n",
    "1. Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective.\n",
    "For each perspective, provide citations in page number\n",
    "\n",
    "2. Ensure that the language and content complexity is appropriate for the specified student age group (if provided).\n",
    "3. If a specific historical timeframe or theme is specified, tailor your responses to fit within those parameters.\n",
    "4. After presenting the perspectives, suggest 2-3 discussion questions that could encourage critical thinking among students about these different viewpoints.\n",
    "\n",
    "Remember, your goal is to provide educators with balanced, well-sourced information that they can use to create engaging and thought-provoking lessons about Singapore's history and culture.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10,\"score_threshold\": 0.5})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a297e928-f804-4d0b-b87c-4fcc9cc98dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing with similarity search with relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bda2ac27-c337-4ef2-a118-fdc3b0aeadf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Raffles was the founder of Singapore as he signed the 1819 Treaty that allowed the British to set up a trading post in the southern part of Singapore. His contributions in establishing Singapore as a thriving settlement are significant.\n",
      "\n",
      "Perspective 2: Farquhar was the founder of Singapore as he did the work of building Singapore from scratch. His efforts in developing the early infrastructure and administration of Singapore were crucial in its foundation.\n",
      "\n",
      "Perspective 3: Crawfurd could be considered the founder of Singapore as he signed the 1824 Treaty of Friendship and Alliance that gave the British control over the whole island. His role in solidifying British control over Singapore was essential in its development.\n",
      "\n",
      "Discussion Questions:\n",
      "1. How do the different perspectives on who founded Singapore reflect the complexities of historical narratives and the importance of multiple viewpoints?\n",
      "2. In what ways do the contributions of Raffles, Farquhar, and Crawfurd highlight the collaborative efforts involved in the founding and development of Singapore?\n",
      "3. How does the concept of founding a place evolve when considering the roles of different individuals in Singapore's history?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific user warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Relevance scores must be between 0 and 1\")\n",
    "\n",
    "\n",
    "def answer_question_from_vector_store(vector_store, input_question, k=5):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions.\n",
    "\n",
    "Generate 3-5 different perspectives on the question, each with a brief summary (2-3 sentences) explaining the reasoning behind that perspective. \n",
    "\n",
    "Please format your response using the following structure:\n",
    "\n",
    "Perspective 1 : (Brief summary of perspective)\n",
    "\n",
    "Perspective 2 : (Brief summary of perspective)\n",
    "\n",
    "[Additional Perspectives if supported by context...]\n",
    "\n",
    "[Discussion Questions]\n",
    "(Only include questions that can be answered using the provided context)\n",
    "1. (question that encourages critical thinking)\n",
    "2. (question that encourages critical thinking)\n",
    "3. (question that encourages critical thinking)\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the query is not relevant to Singapore History, Please reply that you dont know the answer.Only answer questions based on the context, do not hallucinate.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Function to format documents for input into the prompt\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Retrieve documents using similarity search with relevance scores\n",
    "    ranked_results = vector_store.similarity_search_with_relevance_scores(input_question, k=k)\n",
    "\n",
    "    # # Collect relevant documents based on the similarity score\n",
    "    # relevant_docs = []\n",
    "    # for doc, score in ranked_results:\n",
    "    #     doc.metadata[\"similarity_score\"] = score  # Add the similarity score to metadata\n",
    "    #     relevant_docs.append(doc)\n",
    "\n",
    "    # formatted_context = format_docs(relevant_docs)\n",
    "\n",
    "        # Collect relevant documents based on the similarity score (filter out negative scores)\n",
    "    relevant_docs = []\n",
    "    for doc, score in ranked_results:\n",
    "      #  if 0.2 <= score <= 1:  # Only accept valid scores between 0 and 1\n",
    "            doc.metadata[\"similarity_score\"] = score  # Add the similarity score to metadata\n",
    "            relevant_docs.append(doc)\n",
    "\n",
    "    formatted_context = format_docs(relevant_docs)\n",
    "    \n",
    "    # Chain the context and question into a format suitable for the prompt\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    # Get the result from the RAG chain\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "\n",
    "    return {\"answer\": result.content, \"context\": relevant_docs}\n",
    "\n",
    "# Test query\n",
    "question = \"Who is the founder of Singapore ?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a788281-e256-4c6f-9773-782f0cc08868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perspective 1: Singapore became an independent nation due to its separation from Malaysia on 9 August 1965. This perspective emphasizes the historical event that led to Singapore's independence.\n",
      "\n",
      "Perspective 2: Singapore's independence was a result of the challenges it faced as a nation, including the need to survive in a volatile regional climate. This perspective highlights the external factors that influenced Singapore's path to independence.\n",
      "\n",
      "Perspective 3: The decision for Singapore to become an independent nation was met with mixed responses, with concerns raised about its ability to thrive on its own. This perspective focuses on the reactions and doubts surrounding Singapore's independence.\n",
      "\n",
      "Discussion Questions:\n",
      "1. How did the volatile regional climate, particularly the Konfrontasi conflict, impact Singapore's journey to independence?\n",
      "2. In what ways did the concerns raised about Singapore's ability to survive as an independent nation shape its early policies and strategies?\n",
      "3. How did the challenges faced by Singapore as an independent nation influence its long-term survival and success?\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "question = \"What caused Singapore to become Independent?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e00e9-dcee-4c5c-b47f-f75031c836a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- pydantic/json output?\n",
    "#load as markdown and chunk from Markdown\n",
    "# docling - issue from quality of output from PDF file?\n",
    "# reranking - pinecone and \n",
    "# langchain reranker - flash rank reranker - \n",
    "#chunk size/markdown\n",
    "#.structured output extraction langchain - output in json\n",
    "# additional comments/ json one-shot\n",
    "# key 1 perspective 1 \n",
    "# async -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec4b5233-b772-4040-ab44-c2869b7cfb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referenced sources:\n",
      "Page: 122\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6950448127989101\n",
      "\n",
      "Page Content:\n",
      "Within a few weeks, the Independence of Singapore Agreement was \n",
      "signed and Singapore was no longer a part of Malaysia. On 9 August 1965, \n",
      "an emotional Prime Minister Lee announced Singapore’s separation from \n",
      "the Federation in a live televised press conference:\n",
      "Everytime we look back to the moment we signed this \n",
      "document it is for us a moment of anguish. … All my \n",
      "life, my whole adult life, I have believed in merger and \n",
      "the unity of the two territories. We are connected \n",
      "by geography, economics and ties of kinship. It broke \n",
      "everything we stood for.\n",
      "– Adapted from The Straits Times, 10 August 1965\n",
      "113\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n",
      "Page: 112\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6340082736210397\n",
      "\n",
      "Page Content:\n",
      "declare, on behalf of the people of \n",
      "Singapore, that as from today, the \n",
      "16th day of September, 1963, Singapore \n",
      "shall be forever a part of the sovereign \n",
      "democratic and independent State of \n",
      "Malaysia, founded upon the principles \n",
      "of liberty and justice and ever seeking \n",
      "the welfare and happiness of her \n",
      "people in a more just and more \n",
      "equal society.\n",
      " Prime Minister Lee Kuan Yew speaking \n",
      "at the Malaysia Day celebrations at \n",
      "City Hall, 16 September 1963\n",
      "103\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n",
      "Title: We Built A Nation\n",
      "Source: Roots Website\n",
      "Url: https://www.roots.gov.sg/stories-landing/stories/We-built-a-nation/We-Built-a-Nation\n",
      "Similarity_score: 0.6171263113031527\n",
      "\n",
      "Page Content:\n",
      "Singapore became an independent country following its exit from Malaysia on 9 August 1965. Thrust into nationhood amid a volatile regional climate plagued by conflict (Indonesia was waging an undeclared war, the Konfrontasi, to oppose the formation of Malaysia), Singapore's founding generation of leaders focused on cohesiveness to ensure its survival. In the words of Singapore's first President Mr Yusof Ishak â€œIf we are to remain a cohesive people, we must concentrate on the factors which bind us together, and not those which will divide us.\"\n",
      "\n",
      "Page: 124\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6091323185460913\n",
      "\n",
      "Page Content:\n",
      "Singapore’s separation from Malaysia on 9 August 1965 was greeted with \n",
      "mixed responses. Many raised concerns over Singapore’s ability to survive \n",
      "on its own. In the next two chapters, you will learn about the steps \n",
      "Singapore took to cope with the challenges it faced as an independent \n",
      "nation and what it did to ensure its survival in the long term.\n",
      " Headline in The Straits Times, 10 August 1965, proclaiming the separation of Singapore from MalaysiaConclusion\n",
      "115\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n",
      "Page: 100\n",
      "Source: Sec2\n",
      "Similarity_score: 0.6026631144744572\n",
      "\n",
      "Page Content:\n",
      "Federation of Malaya Federation of Malaya if \n",
      "merged with SingaporeFederation of Malaya if \n",
      "merged with Singapore, \n",
      "North Borneo and Sarawak02\n",
      "134Population (million)ChineseMalay3.1m\n",
      "2.3m3.4m3.6m4.0m\n",
      "3.7m\n",
      "91\n",
      "HOW DID SINGAPORE BECOME AN INDEPENDENT NATION?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    # Print all metadata keys and values\n",
    "    #print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        # Check if the key is 'title', 'source', or 'url' and handle them accordingly\n",
    "        if key in ['title', 'source', 'url']:\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "        else:\n",
    "            # Print other metadata normally\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Print the page content as well\n",
    "    print(f\"\\nPage Content:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2e038-dff0-484a-8c38-c6141129b0e3",
   "metadata": {},
   "source": [
    "## Match to each perspective to top source by Cosine Similarity...?\n",
    "##Johnny Tay\n",
    "2:15 PM\n",
    "https://cohere.com/rerank\n",
    "Meldrick Wee\n",
    "2:15 PM\n",
    "https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/\n",
    "Johnny Tay\n",
    "2:16 PM\n",
    "https://python.langchain.com/docs/integrations/retrievers/cohere-reranker/\n",
    "Johnny Tay\n",
    "2:18 PM\n",
    "https://alain-airom.medium.com/my-first-hands-on-experience-with-docling-ed96eb7f864b\n",
    "Johnny Tay\n",
    "2:19 PM\n",
    "https://huggingface.co/tasks/sentence-similarity\n",
    "Johnny Tay\n",
    "2:23 PM\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
    ")\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1b4f9-9cdc-4343-b7ef-c2588935102a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

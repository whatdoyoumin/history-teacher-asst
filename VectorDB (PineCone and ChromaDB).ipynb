{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99b75b-24d3-4c39-b616-a040ace617f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06630f-4b39-47e4-9d61-a6be47a4719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "csv_file = \"articles.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create documents from CSV data\n",
    "articles = []\n",
    "for index, row in df.iterrows():\n",
    "    articles.append(Document(\n",
    "        page_content=row['text'],\n",
    "        metadata={\n",
    "            'title': row['title'],\n",
    "            'source': row['source'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Step 2: Load and Process PDFs\n",
    "pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "pdf_documents = []\n",
    "\n",
    "for label, path in pdf_paths.items():\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    for page in pages:\n",
    "        pdf_documents.append({\n",
    "            \"text\": page.page_content,\n",
    "            \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "        })\n",
    "\n",
    "# Combine both article and PDF documents into one list\n",
    "all_documents = articles + pdf_documents\n",
    "\n",
    "# Step 3: Chunk the Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_documents:\n",
    "    splits = text_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        all_chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "\n",
    "# Step 4: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Step 5: Create the Combined Chroma Vector Store\n",
    "# Chroma requires a collection name for each vector store\n",
    "chroma_vector_store = Chroma.from_texts(\n",
    "    [chunk[\"text\"] for chunk in all_chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in all_chunks],\n",
    "    collection_name=\"combined_documents\"\n",
    ")\n",
    "\n",
    "# Step 6: Save the Combined Vector Store\n",
    "chroma_vector_store.persist()\n",
    "print(\"Combined Chroma vector store created and saved successfully!\")\n",
    "\n",
    "# Function to query from the vector store (example)\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c22dc-f134-4f60-86ab-c08983554b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "question = \"Who is the founder of Singapore?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "print(response['answer'])\n",
    "print()\n",
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    print(f\"Page {doc.metadata['page']} (Source: {doc.metadata['source']}):\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4004e4-6c38-4833-90b6-f758cf8c1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the combined documents (articles + PDF pages) to a file\n",
    "with open('processed_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(all_documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6abde-f9e3-417a-a567-cb199d4ac491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the processed documents from the pickle file\n",
    "with open('processed_documents.pkl', 'rb') as f:\n",
    "    all_documents = pickle.load(f)\n",
    "\n",
    "print(\"Documents loaded from file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbac99-65de-49fb-be20-7951a0c33b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone (use your own API key)\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=\"us-west1-gcp\")  # Use your region\n",
    "\n",
    "# # Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "# csv_file = \"articles.csv\"  # Replace with your CSV file path\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Create documents from CSV data\n",
    "# articles = []\n",
    "# for index, row in df.iterrows():\n",
    "#     articles.append(Document(\n",
    "#         page_content=row['text'],\n",
    "#         metadata={\n",
    "#             'title': row['title'],\n",
    "#             'source': row['source'],\n",
    "#             'url': row['url']\n",
    "#         }\n",
    "#     ))\n",
    "\n",
    "# # Step 2: Load and Process PDFs\n",
    "# pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "# pdf_documents = []\n",
    "\n",
    "# for label, path in pdf_paths.items():\n",
    "#     loader = PyPDFLoader(path)\n",
    "#     pages = loader.load()\n",
    "#     for page in pages:\n",
    "#         pdf_documents.append({\n",
    "#             \"text\": page.page_content,\n",
    "#             \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "#         })\n",
    "\n",
    "# # Combine both article and PDF documents into one list\n",
    "# all_documents = articles + pdf_documents\n",
    "\n",
    "# Step 3: Chunk the Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_documents:\n",
    "    splits = text_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        all_chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "\n",
    "# Step 4: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Step 5: Create the Combined Pinecone Vector Store\n",
    "# Create a unique index name for Pinecone\n",
    "index_name = \"combined_documents\"\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=embeddings.embed_query(\"test\").shape[0])\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone.GRPCIndex(index_name)\n",
    "\n",
    "# Step 6: Add Documents to Pinecone\n",
    "pinecone_docs = [\n",
    "    {\n",
    "        \"id\": str(i),\n",
    "        \"values\": embeddings.embed_documents([chunk[\"text\"]])[0],  # embedding for text\n",
    "        \"metadata\": chunk[\"metadata\"]\n",
    "    }\n",
    "    for i, chunk in enumerate(all_chunks)\n",
    "]\n",
    "\n",
    "# Upsert documents into Pinecone\n",
    "index.upsert(vectors=pinecone_docs)\n",
    "\n",
    "print(\"Combined Pinecone vector store created and saved successfully!\")\n",
    "\n",
    "# Function to query from the vector store (example)\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d99b75b-24d3-4c39-b616-a040ace617f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if a specific variable is loaded\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3a4a5d-079a-4f60-b44b-79c1c0f28a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (2.10.3)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Using cached chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.14.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.20.1-cp312-cp312-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (7.7.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.2.1-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.1.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from chromadb) (0.28.0)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-1.0.4-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-15.0-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\minli\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "Using cached bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
      "Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "Using cached mmh3-5.1.0-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Using cached onnxruntime-1.20.1-cp312-cp312-win_amd64.whl (11.3 MB)\n",
      "Using cached opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
      "Using cached opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
      "Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
      "Using cached posthog-3.14.1-py2.py3-none-any.whl (73 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Using cached googleapis_common_protos-1.67.0-py2.py3-none-any.whl (164 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Using cached huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Using cached watchfiles-1.0.4-cp312-cp312-win_amd64.whl (285 kB)\n",
      "Using cached websockets-15.0-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Using cached fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build chroma-hnswlib\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'hnswlib' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for chroma-hnswlib\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (chroma-hnswlib)\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97f8a86-bcb5-4068-b405-6df9f8ef8668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded from file.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the processed documents from the pickle file\n",
    "with open('textbooks.pkl', 'rb') as f:\n",
    "    pdf_documents = pickle.load(f)\n",
    "\n",
    "print(\"Documents loaded from file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef06630f-4b39-47e4-9d61-a6be47a4719c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_community\\vectorstores\\chroma.py:83\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 66\u001b[0m\n\u001b[0;32m     62\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-small\u001b[39m\u001b[38;5;124m\"\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Step 5: Create the Combined Chroma Vector Store\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Chroma requires a collection name for each vector store\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m chroma_vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_chunks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_chunks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     71\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Step 6: Save the Combined Vector Store\u001b[39;00m\n\u001b[0;32m     74\u001b[0m chroma_vector_store\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_community\\vectorstores\\chroma.py:814\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[0;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_community\\vectorstores\\chroma.py:86\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m     )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m client_settings\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "csv_file = \"roots_sg_articles_cleaned.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Fill NaN values in 'text' column with an empty string or a default string\n",
    "df['text'] = df['text'].fillna('missing content')\n",
    "\n",
    "# Ensure 'text' column is a string\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "\n",
    "# Create documents from CSV data\n",
    "articles = []\n",
    "for index, row in df.iterrows():\n",
    "    articles.append(Document(\n",
    "        page_content=row['text'],\n",
    "        metadata={\n",
    "            'title': row['title'],\n",
    "            'source': row['source'],\n",
    "            'url': row['url']\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# # Step 2: Load and Process PDFs\n",
    "# pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "# pdf_documents = []\n",
    "\n",
    "# for label, path in pdf_paths.items():\n",
    "#     loader = PyPDFLoader(path)\n",
    "#     pages = loader.load()\n",
    "#     for page in pages:\n",
    "#         pdf_documents.append({\n",
    "#             \"text\": page.page_content,\n",
    "#             \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "#         })\n",
    "\n",
    "# Combine both article and PDF documents into one list\n",
    "all_documents = articles + pdf_documents\n",
    "\n",
    "# Step 3: Chunk the Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100,separators=[\".\", \"!\", \"?\", \",\", \"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_documents:\n",
    "    splits = text_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        all_chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "\n",
    "# Step 4: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Step 5: Create the Combined Chroma Vector Store\n",
    "# Chroma requires a collection name for each vector store\n",
    "chroma_vector_store = Chroma.from_texts(\n",
    "    [chunk[\"text\"] for chunk in all_chunks], \n",
    "    embeddings, \n",
    "    metadatas=[chunk[\"metadata\"] for chunk in all_chunks],\n",
    "    collection_name=\"combined_documents\"\n",
    ")\n",
    "\n",
    "# Step 6: Save the Combined Vector Store\n",
    "chroma_vector_store.persist()\n",
    "print(\"Combined Chroma vector store created and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53139ebf-d629-4eda-8035-bae351150bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 428 entries, 0 to 427\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     420 non-null    object\n",
      " 1   text    415 non-null    object\n",
      " 2   source  428 non-null    object\n",
      " 3   title   365 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 13.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21110628-3d7a-4d78-99cd-230babec684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to query from the vector store (example)\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c22dc-f134-4f60-86ab-c08983554b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "question = \"Who is the founder of Singapore?\"\n",
    "response = answer_question_from_vector_store(vectorstore, question)\n",
    "print(response['answer'])\n",
    "print()\n",
    "print(\"Referenced sources:\")\n",
    "for doc in response['context']:\n",
    "    print(f\"Page {doc.metadata['page']} (Source: {doc.metadata['source']}):\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4004e4-6c38-4833-90b6-f758cf8c1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the combined documents (articles + PDF pages) to a file\n",
    "with open('processed_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(all_documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6abde-f9e3-417a-a567-cb199d4ac491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the processed documents from the pickle file\n",
    "with open('processed_documents.pkl', 'rb') as f:\n",
    "    all_documents = pickle.load(f)\n",
    "\n",
    "print(\"Documents loaded from file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbac99-65de-49fb-be20-7951a0c33b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone (use your own API key)\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"))  # Use your region\n",
    "\n",
    "# # Step 1: Load Articles from CSV (assuming columns: url, text, source, title)\n",
    "# csv_file = \"articles.csv\"  # Replace with your CSV file path\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Create documents from CSV data\n",
    "# articles = []\n",
    "# for index, row in df.iterrows():\n",
    "#     articles.append(Document(\n",
    "#         page_content=row['text'],\n",
    "#         metadata={\n",
    "#             'title': row['title'],\n",
    "#             'source': row['source'],\n",
    "#             'url': row['url']\n",
    "#         }\n",
    "#     ))\n",
    "\n",
    "# # Step 2: Load and Process PDFs\n",
    "# pdf_paths = {\"Sec1\": \"sec1.pdf\", \"Sec2\": \"sec2.pdf\"}\n",
    "# pdf_documents = []\n",
    "\n",
    "# for label, path in pdf_paths.items():\n",
    "#     loader = PyPDFLoader(path)\n",
    "#     pages = loader.load()\n",
    "#     for page in pages:\n",
    "#         pdf_documents.append({\n",
    "#             \"text\": page.page_content,\n",
    "#             \"metadata\": {\"page\": page.metadata[\"page\"], \"source\": label}\n",
    "#         })\n",
    "\n",
    "# # Combine both article and PDF documents into one list\n",
    "# all_documents = articles + pdf_documents\n",
    "\n",
    "# Step 3: Chunk the Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_documents:\n",
    "    splits = text_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        all_chunks.append({\n",
    "            \"text\": split,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "\n",
    "# Step 4: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Step 5: Create the Combined Pinecone Vector Store\n",
    "# Create a unique index name for Pinecone\n",
    "index_name = \"combined_documents\"\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=embeddings.embed_query(\"test\").shape[0])\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone.GRPCIndex(index_name)\n",
    "\n",
    "# Step 6: Add Documents to Pinecone\n",
    "pinecone_docs = [\n",
    "    {\n",
    "        \"id\": str(i),\n",
    "        \"values\": embeddings.embed_documents([chunk[\"text\"]])[0],  # embedding for text\n",
    "        \"metadata\": chunk[\"metadata\"]\n",
    "    }\n",
    "    for i, chunk in enumerate(all_chunks)\n",
    "]\n",
    "\n",
    "# Upsert documents into Pinecone\n",
    "index.upsert(vectors=pinecone_docs)\n",
    "\n",
    "print(\"Combined Pinecone vector store created and saved successfully!\")\n",
    "\n",
    "# Function to query from the vector store (example)\n",
    "def answer_question_from_vectorstore(vector_store, input_question):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "You are the Heritage Education Research Assistant, an AI-powered tool designed to help educators in Singapore create comprehensive and balanced lesson plans about Singapore's history and culture. Your task is to provide multiple perspectives on historical questions, with a focus on validated sources from the National Heritage Board (NHB) and other reputable institutions.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnableLambda(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]})\n",
    "        | prompt\n",
    "        | ChatOpenAI(temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "    result = rag_chain_from_docs.invoke({\"context\": formatted_context, \"question\": input_question})\n",
    "    return {\"answer\": result.content, \"context\": retrieved_docs}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
